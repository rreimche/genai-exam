{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDsOM2ykvU+50h2xobWg67",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rreimche/genai-exam/blob/main/SLRev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lOrwSJU5Lyix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites:\n",
        "- Google Colab\n",
        "- Access to a Google Colab Secret named \"HF_API_TOKEN\" containing a Huggingface API token with read access."
      ],
      "metadata": {
        "id": "l1dAEpCOLUHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Settings\n",
        "from google.colab import userdata\n",
        "\n",
        "download_dir = \"papers\"  # directory name to download papers to revie to\n",
        "huggingface_apikey = userdata.get('HF_API_TOKEN')  # use colab secrets to store the huggingface api key\n",
        "groq_key = userdata.get('GROQ_KEY')  #  use colab secrets to store the huggingface api key\n"
      ],
      "metadata": {
        "id": "q-ZPkAZ34Aem",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Declare bibtex references\n",
        "\n",
        "bibtext_references = \"\"\"\n",
        "\n",
        "@misc{rasheed2024codeporilargescaleautonomoussoftware,\n",
        "      title={CodePori: Large-Scale System for Autonomous Software Development Using Multi-Agent Technology},\n",
        "      author={Zeeshan Rasheed and Malik Abdul Sami and Kai-Kristian Kemell and Muhammad Waseem and Mika Saari and Kari Systä and Pekka Abrahamsson},\n",
        "      year={2024},\n",
        "      eprint={2402.01411},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.SE},\n",
        "      url={https://arxiv.org/abs/2402.01411},\n",
        "},\n",
        "@misc{hong2024metagptmetaprogrammingmultiagent,\n",
        "      title={MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework},\n",
        "      author={Sirui Hong and Mingchen Zhuge and Jiaqi Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and Jürgen Schmidhuber},\n",
        "      year={2024},\n",
        "      eprint={2308.00352},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.AI},\n",
        "      url={https://arxiv.org/abs/2308.00352},\n",
        "},\n",
        "\n",
        "\n",
        "@misc{qian2024chatdevcommunicativeagentssoftware,\n",
        "      title={ChatDev: Communicative Agents for Software Development},\n",
        "      author={Chen Qian and Wei Liu and Hongzhang Liu and Nuo Chen and Yufan Dang and Jiahao Li and Cheng Yang and Weize Chen and Yusheng Su and Xin Cong and Juyuan Xu and Dahai Li and Zhiyuan Liu and Maosong Sun},\n",
        "      year={2024},\n",
        "      eprint={2307.07924},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.SE},\n",
        "      url={https://arxiv.org/abs/2307.07924},\n",
        "},\n",
        "\n",
        "@misc{zhang2024empoweringagilebasedgenerativesoftware,\n",
        "      title={Empowering Agile-Based Generative Software Development through Human-AI Teamwork},\n",
        "      author={Sai Zhang and Zhenchang Xing and Ronghui Guo and Fangzhou Xu and Lei Chen and Zhaoyuan Zhang and Xiaowang Zhang and Zhiyong Feng and Zhiqiang Zhuang},\n",
        "      year={2024},\n",
        "      eprint={2407.15568},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.SE},\n",
        "      url={https://arxiv.org/abs/2407.15568},\n",
        "},\n",
        "\n",
        "@misc{nguyen2024agilecoderdynamiccollaborativeagents,\n",
        "      title={AgileCoder: Dynamic Collaborative Agents for Software Development based on Agile Methodology},\n",
        "      author={Minh Huynh Nguyen and Thang Phan Chau and Phong X. Nguyen and Nghi D. Q. Bui},\n",
        "      year={2024},\n",
        "      eprint={2406.11912},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.SE},\n",
        "      url={https://arxiv.org/abs/2406.11912},\n",
        "},\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9OLg8Lv_RdoQ",
        "cellView": "form"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "!pip install -q autogen arxiv scholarly crossrefapi beautifulsoup4 requests cloudscraper pymupdf nltk autogen-agentchat autogen-ext[openai] groq;\n",
        "!pip install -q --pre bibtexparser;"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ONfbxa-ELoqt"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download papers\n",
        "from autogen import UserProxyAgent, config_list_from_json\n",
        "import arxiv\n",
        "import os\n",
        "import datetime\n",
        "from bibtexparser.bparser import BibTexParser\n",
        "from bibtexparser.bibdatabase import BibDatabase\n",
        "\n",
        "def download_papers(paper_info_list: list, download_dir: str) -> dict:\n",
        "    results = {}\n",
        "\n",
        "    for paper_info in paper_info_list:\n",
        "        title = paper_info.get(\"title\", \"\")\n",
        "        results[title] = {\"success\": False, \"filepath\": None}\n",
        "\n",
        "        # 1. Try arXiv (using eprint ID)\n",
        "        eprint_id = paper_info.get(\"eprint\")\n",
        "        if eprint_id:\n",
        "            print(f\"{datetime.datetime.now()} - Trying arXiv for: {title} (arXiv ID: {eprint_id})\")\n",
        "            try:\n",
        "                search = arxiv.Search(id_list=[eprint_id], max_results=1)\n",
        "                result = next(search.results(), None)\n",
        "                if result:\n",
        "                    print(f\"  Found on arXiv: {result.title}\")\n",
        "                    filepath = os.path.join(download_dir, f\"{title.replace(' ', '_')}.pdf\")\n",
        "                    result.download_pdf(filename=filepath)\n",
        "                    results[title][\"success\"] = True\n",
        "                    results[title][\"filepath\"] = filepath\n",
        "                    print(f\"  Download successful: {filepath}\")\n",
        "                else:\n",
        "                    print(f\"  No arXiv result found for ID {eprint_id}.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"arXiv download failed for: {title}: {e}\")\n",
        "\n",
        "        # Check if the download was NOT successful\n",
        "        if not results[title][\"success\"]:\n",
        "            print(f\"{datetime.datetime.now()} - Download failed or not attempted for: {title}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def parse_bibtex_entries(bibtex_refs: str) -> list:\n",
        "\n",
        "    try:\n",
        "        parser = BibTexParser()\n",
        "        bib_database = parser.parse(bibtex_refs)\n",
        "        return bib_database.entries\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing BibTeX string: {e}\")\n",
        "        return []\n",
        "\n",
        "def maybe_create_dir(donwload_dir: str) -> None:\n",
        "    try:\n",
        "        os.makedirs(donwload_dir, exist_ok=True)\n",
        "        print(f\"Directory '{download_dir}' created successfully.\")\n",
        "    except OSError as e:\n",
        "        print(f\"Error creating directory '{download_dir}': {e}\")\n",
        "        exit(1)  # Exit with an error code\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "# 1. Load BibTeX entries\n",
        "\n",
        "papers_to_download = parse_bibtex_entries(bibtext_references)\n",
        "\n",
        "# 2. Download papers (if entries were loaded)\n",
        "if False and papers_to_download:\n",
        "    download_dir = \"papers\"\n",
        "    maybe_create_dir(download_dir)\n",
        "    download_results = download_papers(papers_to_download, download_dir)\n",
        "\n",
        "    # Count successful downloads\n",
        "    successful_downloads = sum(1 for paper_data in download_results.values() if paper_data[\"success\"])\n",
        "    total_papers = len(papers_to_download)\n",
        "\n",
        "    print(f\"Downloaded {successful_downloads} of {total_papers} papers, done.\")  # Summary message\n",
        "\n",
        "else:\n",
        "    print(\"No papers to download.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-YxOZKAZdcl",
        "outputId": "b6efa253-cbeb-4298-bb85-06ffc2e6b8c9",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No papers to download.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parse PDFs to text\n",
        "import fitz  # PyMuPDF\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def parse_papers(download_results: dict, paper_info_list: list) -> dict:\n",
        "    \"\"\"\n",
        "    Parses downloaded PDF papers, extracts text, and indexes by arXiv eprint.\n",
        "\n",
        "    Args:\n",
        "        download_results: Dictionary from download_papers (title -> {success, filepath}).\n",
        "        paper_info_list:  Original list of paper info dictionaries (from BibTeX).\n",
        "\n",
        "    Returns:\n",
        "        A dictionary mapping arXiv eprints to the extracted text and title.\n",
        "        {\n",
        "            \"arxiv_eprint\": {\"text\": \"extracted text...\", \"title\": \"paper title\"},\n",
        "            ...\n",
        "        }\n",
        "        Returns an empty dictionary if there are issues with the input data.\n",
        "    \"\"\"\n",
        "    parsed_texts = {}\n",
        "\n",
        "    # Create a lookup dictionary for paper info by title\n",
        "    title_to_info = {paper_info.get('title'): paper_info for paper_info in paper_info_list}\n",
        "\n",
        "    for title, data in download_results.items():\n",
        "        # We only process successfully downloaded papers\n",
        "        if data[\"success\"]:\n",
        "            filepath = data[\"filepath\"]\n",
        "\n",
        "            # Get eprint from the original paper info using the title lookup\n",
        "            paper_info = title_to_info.get(title)\n",
        "            if not paper_info:\n",
        "                print(f\"Error: Could not find original paper info for title: {title}\")\n",
        "                continue  # Skip this paper\n",
        "\n",
        "            eprint_id = paper_info.get(\"eprint\")\n",
        "            if not eprint_id:\n",
        "                print(f\"Error: No eprint ID found for title: {title}\")\n",
        "                continue  # Skip papers without an eprint\n",
        "\n",
        "            try:\n",
        "                with fitz.open(filepath) as doc:\n",
        "                    text = \"\"\n",
        "                    for page in doc:\n",
        "                        text += page.get_text()\n",
        "                    parsed_texts[eprint_id] = {\"text\": text, \"title\": title}\n",
        "                print(f\"Successfully parsed: {title} (arXiv ID: {eprint_id})\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error parsing {title} ({filepath}): {e}\")\n",
        "                # Don't add to parsed_texts if parsing fails.\n",
        "\n",
        "    return parsed_texts\n",
        "\n",
        "# Example Usage (assuming download_results and papers_to_download are available):\n",
        "parsed_texts = {} # parse_papers(download_results, papers_to_download)\n",
        "print()\n",
        "for eprint, data in parsed_texts.items():\n",
        "    print(f\"---- arXiv eprint: {eprint} ----\")\n",
        "    print(f\"Title: {data['title']}\")\n",
        "    print(f\"Length: {len(data['text'])}\")\n",
        "    print(f\"First 100 characters with newlines stripped:\")\n",
        "    print(data['text'][:100].replace('\\n', ' '))\n",
        "    print(f\"Token count: {len(word_tokenize(data['text']))}\")\n",
        "    print(f\"---- end {eprint} ----\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "L-j3Rc_EjB9B",
        "outputId": "872dabd7-5704-453e-966f-9492604b73ac"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preparation for agents: model client connections\n",
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.messages import TextMessage\n",
        "from autogen_core.tools import FunctionTool\n",
        "from autogen_core.models import UserMessage\n",
        "#from autogen_agentchat.ui import Console\n",
        "from autogen_core import CancellationToken\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "# INSTANTIATE MODEL CLIENTS\n",
        "\n",
        "# This client will be used for paper summarization\n",
        "model_client = OpenAIChatCompletionClient(\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    base_url=\"https://router.huggingface.co/hf-inference/v1\",\n",
        "    api_key=huggingface_apikey,\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": False,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# We need another inference point for downloader agent,\n",
        "# because huggingface can't serve reflection on tool use\n",
        "# so that autogen agent understands it\n",
        "downloader_model_client = OpenAIChatCompletionClient(\n",
        "    model=\"llama3-70b-8192\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=groq_key,\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# CHECK CONNECTION\n",
        "\n",
        "messages = [\n",
        "    UserMessage(content=\"What is the capital of France?\", source=\"user\"),\n",
        "]\n",
        "response = await model_client.create(messages=messages)\n",
        "response_downloader = await downloader_model_client.create(messages=messages)\n",
        "\n",
        "print(response.content)\n",
        "print(response_downloader.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwt9LJMLbU6m",
        "outputId": "d28617a9-2285-4edd-9c1d-b22dea09ddc4"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris. However, it's important to note that Paris is not a constituent country, but rather a city, and France is a sovereign state. The seat of the government is located in Paris, while French law technically considers France to be an indivisible, indissoluble, secular, democratic and social Republic.\n",
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lDnHXcaKV7S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Downloader agent\n",
        "import arxiv\n",
        "import os\n",
        "import datetime\n",
        "import json\n",
        "import sys\n",
        "\n",
        "# HELPER AND TOOL FUNCTIONS\n",
        "\n",
        "def maybe_create_dir(donwload_dir: str) -> None:\n",
        "    \"\"\"\n",
        "        A helper function to create the downloads directory\n",
        "        in colab root if the directory is not yet present.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        os.makedirs(donwload_dir, exist_ok=True)\n",
        "        #print(f\"Directory '{download_dir}' created successfully.\")\n",
        "    except OSError as e:\n",
        "        raise Exception(f\"Error creating directory '{download_dir}': {e}\")\n",
        "\n",
        "def download_paper(eprint_id: str) -> str:\n",
        "    \"\"\"\n",
        "        Receives a eprint_id of a paper from arxiv, downloads the specified paper\n",
        "        and saves it in downloads directory.\n",
        "        The filepath of the downloaded paper is in the resulting dictionary\n",
        "        with the key \"filepath\".\n",
        "\n",
        "        :param eprint_id: string, the arxiv eprint_id.\n",
        "        :return: string with filepath of the downloaded paper.\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(eprint_id) > 0, \"eprint_id cannot be empty\"\n",
        "\n",
        "    result = {\n",
        "        \"eprint_id\": eprint_id,\n",
        "        \"success\": False,\n",
        "        \"filepath\": None,\n",
        "        }\n",
        "\n",
        "\n",
        "    if eprint_id:\n",
        "        #print(f\"{datetime.datetime.now()} - Trying arXiv for: {eprint_id}\")\n",
        "        try:\n",
        "            client = arxiv.Client()\n",
        "            search = arxiv.Search(id_list=[eprint_id], max_results=1)\n",
        "\n",
        "            #arxiv_result = next(search.results(), None)\n",
        "            arxiv_result = next(client.results(search))\n",
        "            if arxiv_result:\n",
        "                #print(f\"  Found on arXiv: {arxiv_result.title}\")\n",
        "                maybe_create_dir(download_dir)\n",
        "                filepath = os.path.join(download_dir, f\"{arxiv_result.title.replace(' ', '_')}.pdf\")\n",
        "                arxiv_result.download_pdf(filename=filepath)\n",
        "                result[\"success\"] = True\n",
        "                result[\"filepath\"] = filepath\n",
        "                #print(f\"  Download successful: {filepath}\")\n",
        "            else:\n",
        "                # print(f\"  No arXiv result found for ID {eprint_id}.\")\n",
        "                raise Exception(f\"ERROR: No arXiv result found for ID {eprint_id}.\")\n",
        "                #return f\"ERROR: No arXiv result found for ID {eprint_id}.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            #print(f\"arXiv download failed for: {eprint_id}: {e}\")\n",
        "            raise Exception(f\"ERROR: arXiv download failed for: {eprint_id}: {e}\")\n",
        "            #return f\"ERROR: arXiv download failed for: {eprint_id}: {e}\"\n",
        "\n",
        "    # Check if the download was NOT successful\n",
        "    if not result[\"success\"]:\n",
        "        #print(f\"{datetime.datetime.now()} - Download failed or not attempted for: {eprint_id}\")\n",
        "        raise Exception(f\"ERROR: Download failed or not attempted for: {eprint_id}\")\n",
        "        #return f\"ERROR: Download failed or not attempted for: {eprint_id}\"\n",
        "\n",
        "    return result[\"filepath\"]\n",
        "\n",
        "\n",
        "# DOWNLOADER AGENT WITH TOOL\n",
        "\n",
        "downloader_tool = FunctionTool(download_paper, description=\"Given a string denoting an eprint_id of a page on arxiv, downloads a paper\")\n",
        "\n",
        "downloader_agent = AssistantAgent(\n",
        "    name=\"downloader\",\n",
        "    model_client=downloader_model_client,\n",
        "    tools=[downloader_tool],\n",
        "    reflect_on_tool_use=True,\n",
        "    system_message=\"\"\"\n",
        "        Use downloader tool to download specified paper(s) from arxiv.\n",
        "        Here is an example of how you must respond in success case:\n",
        "          'mypapers/veryinterstingpaper.pdf'\n",
        "        If download was not successful, respond like this:\n",
        "          'ERROR: Download for paper with eprint id XXX failed for the following reason: REASON'.\n",
        "      \"\"\",\n",
        ")\n",
        "\n",
        "\n",
        "# CHECK IF DOWNLOAD WORKS\n",
        "\n",
        "async def downloader_run() -> None:\n",
        "    response = await downloader_agent.on_messages(\n",
        "        [TextMessage(content=\"Download the paper with eprint_id 2402.01411\", source=\"user\")],\n",
        "        cancellation_token=CancellationToken(),\n",
        "    )\n",
        "\n",
        "\n",
        "    #print(response.inner_messages)\n",
        "    #print(response.chat_message)\n",
        "    print(response.chat_message.content)\n",
        "\n",
        "\n",
        "\n",
        "# Use asyncio.run(assistant_run()) when running in a script.\n",
        "await downloader_run()"
      ],
      "metadata": {
        "id": "I5Zv1Bs32eOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cb30a6e-a6bb-418b-ed54-388411ef4932"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tool has successfully downloaded the paper with eprint_id 2402.01411, and the PDF file is now available at `papers/CodePori:_Large-Scale_System_for_Autonomous_Software_Development_Using_Multi-Agent_Technology.pdf`.\n",
            "\n",
            "You can access the paper by opening this file. The title of the paper is \"CodePori: Large-Scale System for Autonomous Software Development Using Multi-Agent Technology\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parser agent\n",
        "import fitz  # PyMuPDF\n",
        "#import nltk\n",
        "#from nltk.tokenize import word_tokenize\n",
        "\n",
        "#nltk.download('punkt_tab')\n",
        "\n",
        "def parse_pdf(filepath: str) -> str:\n",
        "    try:\n",
        "        with fitz.open(filepath) as doc:\n",
        "            text = \"\"\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "            # print(f\"Successfully parsed: {title} (arXiv ID: {eprint_id})\")\n",
        "            return text\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Error parsing {title} ({filepath}): {e}\")\n",
        "        raise Exception(f\"ERROR: Error parsing {filepath}: {e}\")\n",
        "\n",
        "\n",
        "parser_tool = FunctionTool(\n",
        "    parse_pdf,\n",
        "    description=\"A tool that takes a filepath of a pdf file, parses the file and returns the text\"\n",
        "  )\n",
        "\n",
        "parser_agent = AssistantAgent(\n",
        "    name=\"parser\",\n",
        "    model_client=downloader_model_client,\n",
        "    system_message=\"\"\"\n",
        "        For a given filepath of a scientific paper,\n",
        "        you use the provided tool, which parses the paper and returns text.\n",
        "        Then you return the text in your response.\n",
        "      \"\"\",\n",
        "    tools=[parser_tool]\n",
        ")\n",
        "\n",
        "# CHECK IF PARSING WORKS\n",
        "\n",
        "test_paper_text = \"\"  # we'll need this to test the summarizer\n",
        "async def parser_run() -> None:\n",
        "    response = await parser_agent.on_messages(\n",
        "        [TextMessage(content=\"Parse this paper: papers/CodePori:_Large-Scale_System_for_Autonomous_Software_Development_Using_Multi-Agent_Technology.pdf\", source=\"user\")],\n",
        "        cancellation_token=CancellationToken(),\n",
        "    )\n",
        "\n",
        "\n",
        "    #print(response.inner_messages)\n",
        "    #print(response.chat_message)\n",
        "    test_paper_text = response.chat_message.content\n",
        "    print(paper_text)\n",
        "\n",
        "\n",
        "\n",
        "# Use asyncio.run(assistant_run()) when running in a script.\n",
        "await parser_run()"
      ],
      "metadata": {
        "id": "W39p-AOvGq21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7ad65f5-1272-4bf7-c5d1-6c52f169de51"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Summarizer agent\n",
        "\n",
        "\n",
        "summarizer_agent = AssistantAgent(\n",
        "    name=\"summarizer\",\n",
        "    model_client=model_client,\n",
        "    system_message=\"\"\"\n",
        "        You are a qualified and experience reviewer and critic of scientific papers.\n",
        "        You will be given a paper text and you will summarize it as your response.\n",
        "        You must fit the summarization in 500 words.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "# CHECK IF SUMMARIZING WORKS\n",
        "\n",
        "async def summarizer_run() -> None:\n",
        "    response = await summarizer.on_messages(\n",
        "        [TextMessage(content=test_paper_text, source=\"user\")],\n",
        "        cancellation_token=CancellationToken(),\n",
        "    )\n",
        "\n",
        "\n",
        "    #print(response.inner_messages)\n",
        "    #print(response.chat_message)\n",
        "    print(response.chat_message.content)\n",
        "\n",
        "\n",
        "\n",
        "# Use asyncio.run(assistant_run()) when running in a script.\n",
        "await parser_run()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "ts9tUs-6XyhP",
        "outputId": "adabee9b-7952-4049-8369-3a563f94c390"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "APIStatusError",
          "evalue": "Error code: 413 - {'error': {'message': 'Request too large for model `llama3-70b-8192` in organization `org_01jjek2v3wfwzrcnqevn2ry6gt` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 22759, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIStatusError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-88001cef0627>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Use asyncio.run(assistant_run()) when running in a script.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mparser_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-3d4501338041>\u001b[0m in \u001b[0;36mparser_run\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mtest_paper_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m  \u001b[0;31m# we'll need this to test the summarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparser_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     response = await parser_agent.on_messages(\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mTextMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Parse this paper: papers/CodePori:_Large-Scale_System_for_Autonomous_Software_Development_Using_Multi-Agent_Technology.pdf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"user\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mcancellation_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCancellationToken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen_agentchat/agents/_assistant_agent.py\u001b[0m in \u001b[0;36mon_messages\u001b[0;34m(self, messages, cancellation_token)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mon_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mChatMessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCancellationToken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mResponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32masync\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_messages_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen_agentchat/agents/_assistant_agent.py\u001b[0m in \u001b[0;36mon_messages_stream\u001b[0;34m(self, messages, cancellation_token)\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;31m# STEP 3: Run the first inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0mmodel_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m         async for inference_output in self._call_llm(\n\u001b[0m\u001b[1;32m    749\u001b[0m             \u001b[0mmodel_client\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mmodel_client_stream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_client_stream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen_agentchat/agents/_assistant_agent.py\u001b[0m in \u001b[0;36m_call_llm\u001b[0;34m(cls, model_client, model_client_stream, system_messages, model_context, tools, handoff_tools, agent_name, cancellation_token)\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mmodel_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m             model_result = await model_client.create(\n\u001b[0m\u001b[1;32m    870\u001b[0m                 \u001b[0mllm_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mall_tools\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcancellation_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/autogen_ext/models/openai/_openai_client.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, tools, json_output, extra_create_args, cancellation_token)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mcancellation_token\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mParsedChatCompletion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseModel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChatCompletion\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_beta_client\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParsedChatCompletion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1725\u001b[0m     ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[1;32m   1726\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m         return await self._post(\n\u001b[0m\u001b[1;32m   1728\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m             body=await async_maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mawait\u001b[0m \u001b[0masync_to_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1848\u001b[0m         )\n\u001b[0;32m-> 1849\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m     async def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001b[0m\n\u001b[1;32m   1541\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1543\u001b[0;31m         return await self._request(\n\u001b[0m\u001b[1;32m   1544\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1545\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, stream, stream_cls, retries_taken)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m         return await self._process_response(\n",
            "\u001b[0;31mAPIStatusError\u001b[0m: Error code: 413 - {'error': {'message': 'Request too large for model `llama3-70b-8192` in organization `org_01jjek2v3wfwzrcnqevn2ry6gt` service tier `on_demand` on tokens per minute (TPM): Limit 6000, Requested 22759, please reduce your message size and try again. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
          ]
        }
      ]
    }
  ]
}
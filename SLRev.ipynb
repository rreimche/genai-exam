{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOHxn3Leu5pJ39AYGRbZ5c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rreimche/genai-exam/blob/main/SLRev.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lOrwSJU5Lyix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites:\n",
        "- Google Colab\n",
        "- Access to a Google Colab Secret named \"HF_API_TOKEN\" containing a Huggingface API token with read access."
      ],
      "metadata": {
        "id": "l1dAEpCOLUHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Settings\n",
        "from google.colab import userdata\n",
        "\n",
        "download_dir = \"papers\"  # directory name to download papers to revie to\n",
        "huggingface_apikey = userdata.get('HF_API_TOKEN')  # use colab secrets to store the huggingface api key\n",
        "groq_key = userdata.get('GROQ_KEY')  #  use colab secrets to store the huggingface api key\n"
      ],
      "metadata": {
        "id": "q-ZPkAZ34Aem",
        "cellView": "form"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Declare bibtex references\n",
        "\n",
        "bibtext_references = \"\"\"\n",
        "\n",
        "@misc{rasheed2024codeporilargescaleautonomoussoftware,\n",
        "      title={CodePori: Large-Scale System for Autonomous Software Development Using Multi-Agent Technology},\n",
        "      author={Zeeshan Rasheed and Malik Abdul Sami and Kai-Kristian Kemell and Muhammad Waseem and Mika Saari and Kari Systä and Pekka Abrahamsson},\n",
        "      year={2024},\n",
        "      eprint={2402.01411},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.SE},\n",
        "      url={https://arxiv.org/abs/2402.01411},\n",
        "},\n",
        "@misc{hong2024metagptmetaprogrammingmultiagent,\n",
        "      title={MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework},\n",
        "      author={Sirui Hong and Mingchen Zhuge and Jiaqi Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and Jürgen Schmidhuber},\n",
        "      year={2024},\n",
        "      eprint={2308.00352},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.AI},\n",
        "      url={https://arxiv.org/abs/2308.00352},\n",
        "},\n",
        "\n",
        "\n",
        "@misc{qian2024chatdevcommunicativeagentssoftware,\n",
        "      title={ChatDev: Communicative Agents for Software Development},\n",
        "      author={Chen Qian and Wei Liu and Hongzhang Liu and Nuo Chen and Yufan Dang and Jiahao Li and Cheng Yang and Weize Chen and Yusheng Su and Xin Cong and Juyuan Xu and Dahai Li and Zhiyuan Liu and Maosong Sun},\n",
        "      year={2024},\n",
        "      eprint={2307.07924},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.SE},\n",
        "      url={https://arxiv.org/abs/2307.07924},\n",
        "},\n",
        "\n",
        "@misc{zhang2024empoweringagilebasedgenerativesoftware,\n",
        "      title={Empowering Agile-Based Generative Software Development through Human-AI Teamwork},\n",
        "      author={Sai Zhang and Zhenchang Xing and Ronghui Guo and Fangzhou Xu and Lei Chen and Zhaoyuan Zhang and Xiaowang Zhang and Zhiyong Feng and Zhiqiang Zhuang},\n",
        "      year={2024},\n",
        "      eprint={2407.15568},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.SE},\n",
        "      url={https://arxiv.org/abs/2407.15568},\n",
        "},\n",
        "\n",
        "@misc{nguyen2024agilecoderdynamiccollaborativeagents,\n",
        "      title={AgileCoder: Dynamic Collaborative Agents for Software Development based on Agile Methodology},\n",
        "      author={Minh Huynh Nguyen and Thang Phan Chau and Phong X. Nguyen and Nghi D. Q. Bui},\n",
        "      year={2024},\n",
        "      eprint={2406.11912},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.SE},\n",
        "      url={https://arxiv.org/abs/2406.11912},\n",
        "},\n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9OLg8Lv_RdoQ",
        "cellView": "form"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "!pip install -q autogen arxiv scholarly crossrefapi beautifulsoup4 requests cloudscraper pymupdf nltk autogen-agentchat autogen-ext[openai] groq pymupdf;\n",
        "!pip install -q --pre bibtexparser;"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ONfbxa-ELoqt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preparation for agents: model client connections\n",
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.messages import TextMessage\n",
        "from autogen_core.tools import FunctionTool\n",
        "from autogen_core.models import UserMessage\n",
        "#from autogen_agentchat.ui import Console\n",
        "from autogen_core import CancellationToken\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "# INSTANTIATE MODEL CLIENTS\n",
        "\n",
        "# This client will be used for paper summarization\n",
        "text_model_client = OpenAIChatCompletionClient(\n",
        "    #model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    base_url=\"https://router.huggingface.co/hf-inference/v1\",\n",
        "    api_key=huggingface_apikey,\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": False,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# We need another inference point for downloader agent,\n",
        "# because huggingface can't serve reflection on tool use\n",
        "# so that autogen agent understands it\n",
        "tool_model_client = OpenAIChatCompletionClient(\n",
        "    model=\"llama3-70b-8192\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=groq_key,\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# CHECK CONNECTION\n",
        "\n",
        "messages = [\n",
        "    UserMessage(content=\"What is the capital of France?\", source=\"user\"),\n",
        "]\n",
        "response = await text_model_client.create(messages=messages)\n",
        "response_downloader = await tool_model_client.create(messages=messages)\n",
        "\n",
        "print(response.content)\n",
        "print(response_downloader.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwt9LJMLbU6m",
        "outputId": "b473f806-a677-4fef-e9fd-a50c25b3c55c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris. It's an iconic city known for landmarks such as the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral. Paris is located in the northern part of the country. It's not only a major cultural hub but also a significant center of economy, education, and politics in France and Europe.\n",
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Downloader agent\n",
        "import arxiv\n",
        "import os\n",
        "import datetime\n",
        "import json\n",
        "import sys\n",
        "\n",
        "# HELPER AND TOOL FUNCTIONS\n",
        "\n",
        "def maybe_create_dir(donwload_dir: str) -> None:\n",
        "    \"\"\"\n",
        "        A helper function to create the downloads directory\n",
        "        in colab root if the directory is not yet present.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        os.makedirs(donwload_dir, exist_ok=True)\n",
        "        #print(f\"Directory '{download_dir}' created successfully.\")\n",
        "    except OSError as e:\n",
        "        raise Exception(f\"Error creating directory '{download_dir}': {e}\")\n",
        "\n",
        "def download_paper(eprint_id: str) -> str:\n",
        "    \"\"\"\n",
        "        Receives a eprint_id of a paper from arxiv, downloads the specified paper\n",
        "        and saves it in downloads directory.\n",
        "        The filepath of the downloaded paper is in the resulting dictionary\n",
        "        with the key \"filepath\".\n",
        "\n",
        "        :param eprint_id: string, the arxiv eprint_id.\n",
        "        :return: string with filepath of the downloaded paper.\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(eprint_id) > 0, \"eprint_id cannot be empty\"\n",
        "\n",
        "    result = {\n",
        "        \"eprint_id\": eprint_id,\n",
        "        \"success\": False,\n",
        "        \"filepath\": None,\n",
        "        }\n",
        "\n",
        "\n",
        "    if eprint_id:\n",
        "        #print(f\"{datetime.datetime.now()} - Trying arXiv for: {eprint_id}\")\n",
        "        try:\n",
        "            client = arxiv.Client()\n",
        "            search = arxiv.Search(id_list=[eprint_id], max_results=1)\n",
        "\n",
        "            #arxiv_result = next(search.results(), None)\n",
        "            arxiv_result = next(client.results(search))\n",
        "            if arxiv_result:\n",
        "                #print(f\"  Found on arXiv: {arxiv_result.title}\")\n",
        "                maybe_create_dir(download_dir)\n",
        "                filepath = os.path.join(download_dir, f\"{arxiv_result.title.replace(' ', '_')}.pdf\")\n",
        "                arxiv_result.download_pdf(filename=filepath)\n",
        "                result[\"success\"] = True\n",
        "                result[\"filepath\"] = filepath\n",
        "                #print(f\"  Download successful: {filepath}\")\n",
        "            else:\n",
        "                # print(f\"  No arXiv result found for ID {eprint_id}.\")\n",
        "                raise Exception(f\"ERROR: No arXiv result found for ID {eprint_id}.\")\n",
        "                #return f\"ERROR: No arXiv result found for ID {eprint_id}.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            #print(f\"arXiv download failed for: {eprint_id}: {e}\")\n",
        "            raise Exception(f\"ERROR: arXiv download failed for: {eprint_id}: {e}\")\n",
        "            #return f\"ERROR: arXiv download failed for: {eprint_id}: {e}\"\n",
        "\n",
        "    # Check if the download was NOT successful\n",
        "    if not result[\"success\"]:\n",
        "        #print(f\"{datetime.datetime.now()} - Download failed or not attempted for: {eprint_id}\")\n",
        "        raise Exception(f\"ERROR: Download failed or not attempted for: {eprint_id}\")\n",
        "        #return f\"ERROR: Download failed or not attempted for: {eprint_id}\"\n",
        "\n",
        "    return result[\"filepath\"]\n",
        "\n",
        "\n",
        "# DOWNLOADER AGENT WITH TOOL\n",
        "\n",
        "downloader_tool = FunctionTool(download_paper, description=\"Given a string denoting an eprint_id of a page on arxiv, downloads a paper\")\n",
        "\n",
        "downloader_agent = AssistantAgent(\n",
        "    name=\"downloader\",\n",
        "    model_client=downloader_model_client,\n",
        "    tools=[downloader_tool],\n",
        "    reflect_on_tool_use=True,\n",
        "    system_message=\"\"\"\n",
        "        Use downloader tool to download specified paper(s) from arxiv.\n",
        "        Here is an example of how you must respond in success case:\n",
        "          'mypapers/veryinterstingpaper.pdf'\n",
        "        If download was not successful, respond like this:\n",
        "          'ERROR: Download for paper with eprint id XXX failed for the following reason: REASON'.\n",
        "      \"\"\",\n",
        ")\n",
        "\n",
        "\n",
        "# CHECK IF DOWNLOAD WORKS\n",
        "\n",
        "async def downloader_run() -> None:\n",
        "    response = await downloader_agent.on_messages(\n",
        "        [TextMessage(content=\"Download the paper with eprint_id 2402.01411\", source=\"user\")],\n",
        "        cancellation_token=CancellationToken(),\n",
        "    )\n",
        "\n",
        "\n",
        "    #print(response.inner_messages)\n",
        "    #print(response.chat_message)\n",
        "    print(response.chat_message.content)\n",
        "\n",
        "\n",
        "\n",
        "# Use asyncio.run(assistant_run()) when running in a script.\n",
        "await downloader_run()"
      ],
      "metadata": {
        "id": "I5Zv1Bs32eOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bcad44f-3cdb-4fd4-e34a-bba40fb13fce",
        "cellView": "form"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper with eprint_id 2402.01411 has been successfully downloaded. The paper is titled \"CodePori: Large-Scale System for Autonomous Software Development Using Multi-Agent Technology\" and has been saved as a PDF file in the \"papers\" directory.\n",
            "\n",
            "Filename: CodePori:_Large-Scale_System_for_Autonomous_Software_Development_Using_Multi-Agent_Technology.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Parser agent\n",
        "import pymupdf\n",
        "#import nltk\n",
        "#from nltk.tokenize import word_tokenize\n",
        "\n",
        "#nltk.download('punkt_tab')\n",
        "\n",
        "def parse_pdf(filepath: str) -> str:\n",
        "    try:\n",
        "        with pymupdf.open(filepath) as doc:\n",
        "            text = \"\"\n",
        "            for page in doc:\n",
        "                text += page.get_text()\n",
        "            # print(f\"Successfully parsed: {title} (arXiv ID: {eprint_id})\")\n",
        "            return text\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Error parsing {title} ({filepath}): {e}\")\n",
        "        raise Exception(f\"ERROR: Error parsing {filepath}: {e}\")\n",
        "\n",
        "\n",
        "parser_tool = FunctionTool(\n",
        "    parse_pdf,\n",
        "    description=\"A tool that takes a filepath of a pdf file, parses the file and returns the text\"\n",
        "  )\n",
        "\n",
        "parser_agent = AssistantAgent(\n",
        "    name=\"parser\",\n",
        "    model_client=tool_model_client,\n",
        "    system_message=\"\"\"\n",
        "        For a given filepath of a scientific paper,\n",
        "        you use the provided tool, which parses the paper and returns text.\n",
        "        Then you return the text in your response.\n",
        "      \"\"\",\n",
        "    tools=[parser_tool]\n",
        ")\n",
        "\n",
        "# CHECK IF PARSING WORKS\n",
        "\n",
        "test_paper_text = \"\"  # we'll need this to test the summarizer\n",
        "async def parser_run() -> None:\n",
        "    response = await parser_agent.on_messages(\n",
        "        [TextMessage(content=\"Parse this paper: papers/CodePori:_Large-Scale_System_for_Autonomous_Software_Development_Using_Multi-Agent_Technology.pdf\", source=\"user\")],\n",
        "        cancellation_token=CancellationToken(),\n",
        "    )\n",
        "\n",
        "\n",
        "    #print(response.inner_messages)\n",
        "    #print(response.chat_message)\n",
        "    text = response.chat_message.content\n",
        "    #print(text)\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "# Use asyncio.run(assistant_run()) when running in a script.\n",
        "test_paper_text = await parser_run()\n",
        "print(test_paper_text)"
      ],
      "metadata": {
        "id": "W39p-AOvGq21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9692bcc-64b3-4554-f718-c1648eee9d24",
        "collapsed": true
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CodePori: Large-Scale System for Autonomous Software\n",
            "Development Using Multi-Agent Technology\n",
            "Zeeshan Rasheedb, Abdul Malik Samib,, Kai-Kristian Kemellb, Muhammad Waseemb, Mika\n",
            "Saarib, Kari Syst¨ab, Pekka Abrahamssonb\n",
            "aFaculty of Information Technology and Communication Science, Tampere University\n",
            "bFaculty of Information Technology, Jyv¨askyl¨a University\n",
            "Abstract\n",
            "Context: Large Language Models (LLMs) and Generative Pre-trained Transformers (GPTs)\n",
            "have transformed the field of Software Engineering (SE). Existing LLM-based multi-agent mod-\n",
            "els have successfully addressed basic dialogue tasks. However, the potential of LLMs for more\n",
            "challenging tasks, such as automated code generation for large and complex projects, has been\n",
            "investigated in only a few existing works.\n",
            "Objective: This paper aims to investigate the potential of LLM-based agents in the software\n",
            "industry, particularly in enhancing productivity and reducing time-to-market for complex soft-\n",
            "ware solutions. Our primary objective is to gain insights into how these agents can fundamen-\n",
            "tally transform the development of large-scale software.\n",
            "Methods: We introduce CodePori, a novel system designed to automate code generation for\n",
            "large and complex software projects based on functional and non-functional requirements de-\n",
            "fined by stakeholders through natural language prompts. We utilized LLM-based multi-agents,\n",
            "where each agent engages with a specific task, including system design, code development, code\n",
            "review, code verification, and test engineering. To assess the proposed system performance,\n",
            "we utilized the HumanEval benchmark and manually tested the CodePori model, providing\n",
            "20 different project descriptions as input and then evaluated the code accuracy by manually\n",
            "executing the code.\n",
            "Results: CodePori is able to generate running code for large-scale projects, aligned with the\n",
            "typical software development process within minutes, and at a cost of a few dollars. The Hu-\n",
            "manEval benchmark results indicate that CodePori improves code accuracy and efficiency by\n",
            "89%. A manual assessment conducted by the first author shows that the CodePori system\n",
            "achieved an accuracy rate of 85%.\n",
            "Conclusion: Based on the results, our conclusion is that CodePori provides a contribution\n",
            "to both industry and academia by automating software development. Additionally, our study\n",
            "demonstrates the transformative potential of LLM-based agents in SE, highlighting their prac-\n",
            "tical applications and opening new opportunities for broader adoption in both industry and\n",
            "academia. This suggests a paradigm shift in how software is developed and deployed. Our\n",
            "project is publicly available at https://github.com/GPT-Laboratory/CodePori.\n",
            "Keywords:\n",
            "OpenAI, AutoGPT, Artificial Intelligence, Natural Language Processing,\n",
            "Generative AI, Software Engineering\n",
            "Email addresses: zeeshan.rasheed@tuni.fi (Zeeshan Rasheed), malik.sami@tuni.fi (Abdul Malik\n",
            "Sami), kai-kristian.kemell@helsinki.fi (Kai-Kristian Kemell), muhammad.m.waseem@jyu.fi (Muhammad\n",
            "Waseem), mika.saari@tuni.fi (Mika Saari), kari.systa@tuni.fi (Kari Syst¨a), pekka.abrahamsson@tuni.fi\n",
            "(Pekka Abrahamsson)\n",
            "Preprint submitted to Information of Software Technology\n",
            "September 18, 2024\n",
            "arXiv:2402.01411v2  [cs.SE]  17 Sep 2024\n",
            "1. Introduction\n",
            "Traditionally, Software Engineering (SE) has mostly depended on human effort, including\n",
            "requirements gathering, designing systems, writing code, and conducting several types of tests.\n",
            "The integration of Artificial Intelligence (AI) into the field of Software Engineering (SE) has\n",
            "resulted in significant advancements and opened up new possibilities [1]. Recent advancements\n",
            "in AI have given rise to advanced Natural Language Processing (NLP) models like the Gener-\n",
            "ative Pre-trained Transformer (GPT) series [2, 3, 4, 5], which are commonly known as Large\n",
            "Language Models (LLMs). LLMs have expanded the range of possibilities across various fields,\n",
            "including SE practices. In contrast to traditional support in software development, interact-\n",
            "ing with an LLM-enhanced development environment involves using natural conversations and\n",
            "text to guide the process, rather than giving specific commands like in traditional systems [6].\n",
            "In this setup, the developer interacts with the system by asking questions or describing the\n",
            "required outcome, rather than detailing the methods for achieving it.\n",
            "The integration of LLMs into SE has transformed the paradigm in this field [7]. LLMs have\n",
            "shown significant benefits compared to traditional methods [8], for instance, models guided\n",
            "by domain-specific languages.\n",
            "LLMs have shown considerable advantages over traditional\n",
            "approaches, such as those driven by domain-specific languages [9]. Hu et al. [10] believe that\n",
            "integrating LLMs will result in a transformative change in software development practices.\n",
            "Within this framework of “LLM-assisted SE”, the LLM collaborates with a range of other\n",
            "supportive bots and tools, providing comprehensive support to human developers throughout\n",
            "every stage of the software lifecycle [11]. In recent years, LLMs have been applied to a variety\n",
            "of fields, including text classification [12], data analysis [13], software development [14], code\n",
            "search [15], etc. Among all development activities in SE, code generation has gained significant\n",
            "attention due to its potential to reduce development costs [16]. As LLMs become increasingly\n",
            "central to software development, a variety of techniques have been developed for LLM-based\n",
            "code generation [17].\n",
            "Considering the complexity of software development, LLM-based agents are more prominent\n",
            "compared to other LLM techniques.\n",
            "Agents are LLM instances, customized to carry out\n",
            "multiple specific tasks, replacing human workflows [18]. Multi-agent systems have advanced\n",
            "significantly in addressing complex challenges in software development by replacing various\n",
            "development roles [18].\n",
            "However, current models continue to face challenges in achieving\n",
            "effective and accurate problem-solving processes [19]. This is particularly evident in scenarios\n",
            "that demand meaningful collaborative interaction among LLM-based multi-agent systems to\n",
            "achieve accurate results, such as generating code for large and complex projects.\n",
            "To this end, we introduce CodePori, a multi-agent system that autonomously generates\n",
            "code for large and complex projects by providing high-level descriptions as input. CodePori\n",
            "represents a step forward in multi-agent systems for SE. It utilizes a multi-agent system where\n",
            "each agent collaboratively contributes to the task of code generation. Each agent is designed\n",
            "to specialize in different aspects of software development, from understanding requirements\n",
            "to writing and optimizing code, thus taking on different roles in the collaboration process.\n",
            "This specialization approach contributes to the effectiveness of the system, especially for large\n",
            "software projects. CodePori has the potential to help tackle challenges commonly faced in\n",
            "large-scale software development, as we demonstrate in this paper.\n",
            "To validate CodePori, we used the publicly available HumanEval benchmark [20]. We com-\n",
            "pared CodePori with recently developed existing models for code generation field, including\n",
            "MetaGPT [19], ChatDev [21], AlphaCode [22], Incoder [23], CodeGeeX [24], Codex [20] and\n",
            "general domain LLMs such as PaLM [25]. The results show that CodePori achieved 89% code\n",
            "accuracy on the HumanEval benchmark. We also manually evaluated the performance of Code-\n",
            "Pori by providing 20 input descriptions of various projects. The first author manually executed\n",
            "the generated code and analyzed the results. The results indicate that CodePori achieved a\n",
            "code accuracy of 85% based on manual assessment. Our contribution can be summarized as\n",
            "follows:\n",
            "2\n",
            "• We designed and developed a multi-agent system called CodePori, which autonomously\n",
            "generates code for large projects from high-level descriptions.\n",
            "• We tested the performance of CodePori using the HumanEval benchmark and manual\n",
            "evaluation. We publicly released a dataset that can help researchers and practitioners\n",
            "access all the collected data for validating our study [26].\n",
            "• The results indicate that the code generated by CodePori demonstrates 89% accuracy on\n",
            "the HumanEval benchmark using the pass@1 metric and 85% accuracy in manual testing\n",
            "with 20 input descriptions across various projects.\n",
            "The rest of the paper is organized as follows. We provide the study methodology in Section\n",
            "2. The results of this study are presented in Section 3, followed by a discussion in Section\n",
            "4. Related work is reviewed in Section 5, and threats to validity are discussed in Section 6.\n",
            "Finally, the study is concluded with future work in Section 7.\n",
            "2. Methodology\n",
            "In this section, we present the methodology for automating the code generation using\n",
            "CodePori system that integrates multiple LLM-based agents. Section 2.1 provides details of\n",
            "the formulated Research Questions (RQs). The multi-agent workflow, structured communica-\n",
            "tion, and their roles are discussed in Section 2.2, and we discuss the details of our evaluation\n",
            "framework in Section 2.3.\n",
            "2.1. Research Questions\n",
            "Based on our study goal, we formulated the following two Research Questions (RQs).\n",
            "RQ1. Can a multi-agent system autonomously generate code, and is it capable of\n",
            "handling the generation of large and complex systems?\n",
            "Objective. The main goal of RQ1 is to investigate the potential of LLM-based multi-agent\n",
            "in the software industry, particularly in enhancing productivity and reducing the time-to-\n",
            "market for complex software solutions. The primary aim is to understand whether LLM-based\n",
            "multi-agent systems can effectively address the complex and modern demands of programming.\n",
            "RQ2. How effective and accurate is the CodePori system in autonomous code genera-\n",
            "tion?\n",
            "Objective. RQ2 aims to validate the effectiveness of CodePori against existing systems.\n",
            "RQ2 explores the comparative strengths of the proposed system within the context of existing\n",
            "solutions, aiming to measure code accuracy. We used the publicly available HumanEval [20]\n",
            "benchmark and manually assessed the performance of CodePori.\n",
            "2.2. CodePori: Multi-agent System\n",
            "CodePori is a multi-agent system with the goal of automating the code generation process\n",
            "for large-scale projects. As shown in Figure 1, we utilized six LLM-based multi-agents to work\n",
            "collaboratively to autonomously perform coding tasks. The multi-agent system, as illustrated\n",
            "in Table 1 of the manuscript, outlines each agent’s profile, including their name, role, goal, and\n",
            "prompt content. Each agent is specialized in specific tasks. More details of these six agents\n",
            "are explained in Table 2. Below, we provide details on how the LLM-based multi-agents were\n",
            "set up and the types of prompts we used to explain the tasks. We also provide the technical\n",
            "details of the proposed algorithms, their functions, and the process of each agent in detail.\n",
            "3\n",
            "Request\n",
            "Response\n",
            "Requests\n",
            "Response\n",
            "Final \n",
            "code\n",
            "Multi-agents\n",
            "framework\n",
            "Manager\n",
            "Agent\n",
            "Dev-1\n",
            "Agent\n",
            "Dev-2\n",
            "Agent\n",
            "Finalized 1 Agent\n",
            "Finalized 2 Agent\n",
            "Embedding\n",
            "API\n",
            "Split as\n",
            "vector\n",
            "Text in smaller\n",
            "chunks\n",
            "LLM API\n",
            "Verification Agent\n",
            "Project Description\n",
            "Figure 1: Workflow diagram showcasing an AI-driven multi-agent system for automated code generation\n",
            "Table 1: Multi-agents main tasks\n",
            "No\n",
            "Agent\n",
            "Main Task\n",
            "Prompt Content\n",
            "1\n",
            "Manager Agent\n",
            "Task Segmentation\n",
            "Take project description and segments these tasks\n",
            "into smaller and structured chunks\n",
            "2\n",
            "Dev 01 Agent\n",
            "Initial Code Generation\n",
            "Generate initial code based on given description\n",
            "3\n",
            "Dev 02 Agent\n",
            "Optimize Generated Code\n",
            "Take the initial generated code from Dev 01 Agent\n",
            "and refine and optimize the generated code\n",
            "4\n",
            "Finalized 01 Agent\n",
            "Code Quality Assurance\n",
            "Play a quality assurance role and refine code for\n",
            "final use\n",
            "5\n",
            "Finalized 02 Agent\n",
            "Review Code Quality\n",
            "Suggest an iterative review process to enhance code\n",
            "quality\n",
            "6\n",
            "Verification Agent\n",
            "Final Verification of Code\n",
            "Focus on identifying and rectifying any flaws in\n",
            "code that the above-mentioned agent might have\n",
            "missed\n",
            "2.2.1. Prompt Engineering\n",
            "To craft the prompts, we formulated an instruction-based prompt following the prompting\n",
            "guidelines [4], which involved identifying the role, explaining the task, and defining the output\n",
            "format for each agent. Each agent’s prompted specific, task-oriented instructions that guided\n",
            "how each agent should interpret the input data, process it, and generate the desired output.\n",
            "For instance, the Manager agent’s role is to take the input description and determine how many\n",
            "modules the project will contain, with each module being limited to a maximum of 200 lines\n",
            "of code. The tasks of the remaining five agents involve generating and refining the code based\n",
            "on ongoing discussions and feedback. All six agents were instructed to communicate with each\n",
            "other and provide feedback based on their respective areas of expertise. We instructed each\n",
            "agent to iterate with the others at least three times to refine and update the code based on\n",
            "feedback from the previous agents, ensuring a thorough and collaborative development process.\n",
            "In Table 2, we have provided the instruction-based prompts for all six agents. For more detailed\n",
            "information about the agents, please refer to our GitHub repository [27].\n",
            "2.2.2. Technical Background\n",
            "In this section, we provide the technical details of the CodePori system. The Algorithm\n",
            "01 shows a comprehensive process for autonomously generating, verifying, and finalizing large\n",
            "project code based on a given description. As we can see, Algorithm 01 uses six LLM-based\n",
            "agents to automate the code generation process. Algorithm 01 is the main algorithm, which\n",
            "connects to six agents and makes API calls based on each agent’s requests. The proposed\n",
            "system code is available on GitHub [27].\n",
            "As shown in Algorithm 01, the procedure begins with the initialization of the API, where\n",
            "the OpenAI API key and system are configured to ensure authenticated and accurate API calls.\n",
            "The project description is transmitted over Hypertext Transfer Protocol Secure (HTTPS) to\n",
            "the OpenAI API server, which ensures data security. Upon receipt, the server validates the\n",
            "API key, confirming its validity, current status, and authorization for the requested task [28].\n",
            "4\n",
            "Once validated, the server processes the request and responds with the requested data [29].\n",
            "API requests are made during each iteration of discussions between the six agents to generate\n",
            "code.\n",
            "The main execution begins by reading the project description from a file. The module\n",
            "descriptions are obtained by calling the get module descriptions function. The Manager agent\n",
            "received the input description and broke down the project description into manageable modules.\n",
            "These manageable modules are distributed among the other agents. Manager agent efficiently\n",
            "segments these tasks into smaller, structured chunks, where the agents identify all the modules\n",
            "or components that need development. The Manager agent sets a limit of 200 lines of code\n",
            "per module. If this limit is exceeded, the agent automatically creates additional modules to\n",
            "accommodate the remaining code. MetaGPT [19] also divides tasks into multiple modules,\n",
            "using multiple agents for collaboration and further subdivision into submodules. However,\n",
            "this approach in MetaGPT is less efficient due to the absence of a centralized coordination\n",
            "mechanism, leading to potential overlaps and inefficiencies in task allocation and execution.\n",
            "The next step involves transforming the task specifications into executable code segments.\n",
            "As shown in Algorithm 01, the Dev-01 agent is tasked with initial code generation, un-\n",
            "dertaking complex development tasks and possibly setting the standard for coding practices.\n",
            "As depicted in Table 1, Dev-01 agent’s primary task is to convert the project specifications,\n",
            "provided by the Manager agent, into initial executable code segments. This agent ensures\n",
            "that each module is developed with efficient code. The initial phase involves the Dev-01 agent\n",
            "drafting the primary code for the module. The generated code must be free of placeholders,\n",
            "with all elements fully implemented. The agent is instructed to use Google-style doc-strings for\n",
            "all functions and classes, ensuring clarity and maintainability of the code. Each code segment\n",
            "must be production-ready, meaning it should be executable and meet all project requirements\n",
            "without further modifications. Dev-02 agent main goal is to ensure efficient code generation.\n",
            "The main component of the Algorithm 01 involves a systematic interaction between Dev 1\n",
            "and Dev 2 agents, whose primary task is to produce the initial code and refine it through three\n",
            "iterations of communication. In each round, Dev 1 sends a message, which Dev 2 responds\n",
            "message together with feedback. The conversation histories are updated accordingly. This\n",
            "iterative interaction main purpose is to refine the code through collaborative exchanges. The\n",
            "final Python code is extracted from the last responses, ensuring that the best possible version\n",
            "of the code is obtained through this collaborative process.\n",
            "The next step is to verify the generated code to ensure quality assurance. The Finalize 01\n",
            "agent received the code from Dev 2 agent and further analyzes the code. The Finalized 1\n",
            "agent’s primary task is to update the current module code to ensure it meets the project\n",
            "description. The Finalized 1 agent starts by understanding the overall project description,\n",
            "which provides the necessary context for the module being developed. Next, Finalized 1 agent\n",
            "is provided with the current module code, which has been developed by Dev-02 agent. The\n",
            "agent then receives a review of the current module code. This review contains detailed feedback\n",
            "on what needs to be changed or improved to make the code production-ready. Finalized 1\n",
            "agent thoroughly understands the existing code and the specific improvements required. This\n",
            "step ensures that all necessary changes are identified and planned. Once Finalized 1 agent\n",
            "is clear on the required improvements, the agent collaborates with Finalized 2 agent to refine\n",
            "the code. The communication structure involves Finalized 1 agent providing Finalized 2 agent\n",
            "with the initial code and requesting further improvements. Finalize 01 and Finalize 02 agent\n",
            "also make two rounds of communication to perform the quality assurance of code. Finalized 02\n",
            "agent begins with receiving the current module code, which has been developed in previous\n",
            "stages, followed by a review of that module code. This review contains detailed feedback on\n",
            "what needs to be changed or improved to make the code production-ready. Finalized 02 agent\n",
            "makes sure that module code did not contain any placeholders; every part of the module must\n",
            "be fully implemented and functional. Each response from Finalized 02 agent must include the\n",
            "updated code without any placeholders or skipped functionality, written in a manner so that\n",
            "it can be copied and executed without modifications.\n",
            "5\n",
            "Table 2: Prompt of Different Agents\n",
            "Manager Agent Prompt\n",
            "Dev 01 Agent Prompt\n",
            "Hey, your name is Manager agent. Your primary role is to act as the project Manager for a complex and lengthy software\n",
            "development project......\n",
            "PROJECT DESCRIPTION Your task is to scrutinize this description, carefully identify the all the modules or components\n",
            "that needs to be developed, and create a clear, concise description for each module. This process is crucial as it will lay the foundation\n",
            "for the structured development of the project...\n",
            "........\n",
            "Here’s a suggested structure for your output for each module:\n",
            "- Module Name:\n",
            "- Detailed Description:\n",
            "- Objective:\n",
            "- Expected Inputs:\n",
            "- Expected Outputs:\n",
            "- Dependencies (if any):\n",
            "- Additional Notes:\n",
            "- Emphasis on Good Practices:\n",
            "Proceed to dissect the project and provide the detailed descriptions for each module to facilitate the step-by-step development of the project.\n",
            "Your insightful breakdown and detailed descriptions are pivotal in ensuring a streamlined development process......\n",
            ".......\n",
            ".......\n",
            "Make sure you return the data in the json format only. with following schema\n",
            "{\n",
            "module 1: {\n",
            "name: ””,\n",
            "detailed description:””,\n",
            "objective: ””,\n",
            "expected inputs: ””,\n",
            "expected outputs: ””,\n",
            "Dependencies: ””,\n",
            "Additional Notes: ””,\n",
            "Emphasis on Good Practices: ””\n",
            "}\n",
            "....\n",
            "}\n",
            "Hey, your name is Dev 1 You are a senior python developer with expertise in clean coding, design patterns, solid principles, good architecture, and\n",
            "everything related to software development in Python.\n",
            "You are part of a collaborative effort to develop a complex software project, module by module, as broken down by Manager agent. Your primary role\n",
            "is to work with Dev 2, another senior python developer, to ensure that each module is developed efficiently, adhering to the highest coding standards\n",
            "and is production level.\n",
            "here’s a short description of project you are working on:\n",
            "PROJECT DESCRIPTIONS\n",
            "far, the following modules have been completed:\n",
            "ACCUMULATED CODE\n",
            "You are now working on the following module:\n",
            "MODULE DESCRIPTION\n",
            "Remember that this module is a part of a bigger project.\n",
            "You need to make sure that the module that you are developing aligns with the modules that are already completed.\n",
            "Following are the most important instruction and must be followed in any case:\n",
            "1. No placeholders in the code (Each module you work should not contain any placeholders it should contain each and every piece)\n",
            "2. Use google docstrings for functions and classes\n",
            "3. Make sure at the code is production level and can be executed without having to make any changes\n",
            "4. You should always write the complete code without any placeholders or TODO’s\n",
            "5. you should always provide new code to the Dev 2\n",
            "6. The number of lines for each code you write should be at least 250.\n",
            "7. In each iteration you should write a code that’s is final and robust from your side.\n",
            "you must conversed in the following structure only:\n",
            "Response:\n",
            "Reflection:\n",
            "Code(final code if no change is required):\n",
            "Critique:\n",
            "Dev 2 Agent Prompt\n",
            "Finalized 01 Agent Prompt\n",
            "Hey, your name is Dev 2 You are a senior python developer with expertise in clean coding, design patterns, solid principles, good architecture, and\n",
            "everything related to software development in Python.\n",
            "You are part of a collaborative effort to develop a complex software project, module by module, as broken down by Manager agent. Your primary role\n",
            "is to work with Dev 1, another senior python developer, to ensure that each module is developed efficiently, adhering to the highest coding standards\n",
            "and code is production level. Dev 1 will provide you with the initial code. If not, then you should write the initial code and provide Dev 1 with that.\n",
            "here’s a short description of project you are working on:\n",
            "PROJECT DESCRIPTION\n",
            "So far, the following modules have been completed:\n",
            "ACCUMULATED CODE\n",
            "You are now working on the following module:\n",
            "MODULE DESCRIPTION\n",
            "Remember the this module is a part of a bigger project.\n",
            "You need to make sure that the module that you are developing aligns with the modules that are already completed.\n",
            "Following are the most important instruction and must be followed in any case:\n",
            "1. No placeholders or TODO’s in the code (Each and every part should be thoroughly completed no TODO’s or placeholder should be there in the code)\n",
            "2. Use google doc-strings for functions and classes\n",
            "3. Make sure at the code is production level and can be executed without having to make any changes\n",
            "4. You should always write the complete code without any placeholders or TODO’s\n",
            "5. you should alway provide new code to the Dev 1\n",
            "6. The number of lines for each code you write should be at least 250.\n",
            "7. In each iteration you should write a code that’s is final and robust from your side.\n",
            "you must conversed in the following structure only:\n",
            "Response:\n",
            "Reflection:\n",
            "Code(final code if no change is required):\n",
            "Critique:\n",
            "Hey your name is bot 1. You are senior python developer who masters in building production ready code.\n",
            "your job is to update the current module code only and make it production level.\n",
            "Here’s a workflow:\n",
            "1. you will be provided with the project description\n",
            "2. Then you will be provided with the current module code\n",
            "3. Then you will receive a review of the current module code (It will contain what needs to be changed to make code production ready)\n",
            "4. Then you should understand the existing code and the improvement that needs to be made.\n",
            "5. Once you are sure about the changes that needs to be made you should work with bot 2 improve the code.\n",
            "How you will communicate with bot 2:\n",
            "1. Provide him with the initial code and ask him to make improvements\n",
            "2. Then take a look at improved code make some more improvement and give the updated code to the bot 2.\n",
            "3. Keep working until you guys think that the current module code is production ready.\n",
            "Strict Instruction:\n",
            "1. The final current module code should not contain any placeholders.\n",
            "2. Each part of the current module must be implemented fully and it should be functional.\n",
            "3. In each response you must write the updated code without any placeholders or skipping any functionality.\n",
            "4. Code should be written in such manner that even if just copied and pasted it should work.\n",
            "5. Again no placeholders like existing code, TODO or anything. no assumption, no skipping.\n",
            "Here’s the project description:\n",
            "PROJECT DESCRIPTION\n",
            "Here’s the current module code:\n",
            "MODULE CODE\n",
            "Here’s the review:\n",
            "REVIEW\n",
            "Finalized 2 Agent Prompt\n",
            "your job is to update the current module code only and make it production level.\n",
            "Hey your name is bot 2. You are senior python developer who masters in building production ready code.\n",
            "your job is to update the current module code only and make it production level.\n",
            "Here’s a workflow:\n",
            "1. you will be provided with the project description\n",
            "2. Then you will be provided with the current module code\n",
            "3. Then you will receive a review of that module code (It will contain what needs to be changed to make code production ready)\n",
            "4. Then you should understand the existing code and the improvement that needs to be made.\n",
            "5. Once you are sure about the changes that needs to be made you should work with bot 1 improve the code.\n",
            "How you will communicate with bot 1:\n",
            "1. You will provided with the initial code and instructions to improve the code.\n",
            "2. Then take a look at code make some more improvement and give the updated code to the bot 1.\n",
            "3. Keep working until you guys think that the code is production ready.\n",
            "Strict Instruction:\n",
            "1. The final current module code should not contain any placeholders.\n",
            "2. Each part of the current module must be implemented fully and it should be functional.\n",
            "3. In each response you must write the updated code without any placeholders or skipping any functionality.\n",
            "4. Code should be written in such manner that even if just copied and pasted it should work.\n",
            "5. Again no placeholders like existing code, TODO or anything. no assumption, no skipping.\n",
            "Here’s the project description:\n",
            "PROJECT DESCRIPTION\n",
            "Here’s the current module code:\n",
            "MODULE CODE\n",
            "Here’s the review:\n",
            "REVIEW\n",
            "Hey you are a senior python developer who has built thousands of production level python applications.\n",
            "I will provide you with the project description. the existing code and current module code.\n",
            "You should analyze the code base carefully and understand what changes needs to be made in the current module code to make it\n",
            "work as if in a production level. Once you understand what changes needs to be done prepare the report for the current module.\n",
            "1. Verify that the code quality is up to production standards.\n",
            "2. Ensure that all modules can successfully interact with each other.\n",
            "3. Check for any placeholders or ”TODO” comments that may have been left in the code.\n",
            "4. Don’t give the updated code just write instructions on what needs to be improved to make product work smoothly.\n",
            "Here’s the project description:\n",
            "PROJECT DESCRIPTION\n",
            "You are expected to be strict in your review to ensure the code is of the highest quality. Below is the code for the pre-existing modules.\n",
            "“‘python\n",
            "ACCUMULATED CODE\n",
            "Here’ the current module named MODULE NAME and the code you have to generate report for this code:\n",
            "“‘python\n",
            "MODULE CODE\n",
            "For final verification, a Verification agent reviews the final code provided by Finalized 02\n",
            "agent. Verification agent’s primary responsibility is to ensure that the module code meets the\n",
            "highest standards of quality and functionality, guaranteeing smooth integration and operation\n",
            "within the overall project. Upon receiving the project description and the current module code,\n",
            "the Verification agent begins a careful analysis of the source code. As shown in Algorithm 01,\n",
            "the verification process begins by initializing the conversation history with project description\n",
            "and final code. An API call is made to the Verification agent, and the response is parsed to\n",
            "extract and return a review. This step ensures that the generated code meets quality standards\n",
            "to specified requirements. Finally, the module code saved into a file and completion messages\n",
            "and the current time are printed to indicate the end of the process.\n",
            "2.3. Evaluation Process\n",
            "In this section, we discuss the details of our evaluation framework. We utilized the Hu-\n",
            "manEval benchmark to test the ability of CodePori. We start by defining the HumanEval\n",
            "benchmark and then discuss the pass@k metric.\n",
            "We also discuss how we manually tested\n",
            "CodePori by providing 20 different project descriptions as input and then evaluating the code’s\n",
            "accuracy by manually executing the generated code. Additionally, we analyzed the output to\n",
            "ensure it aligned with the expected functionality and identified any discrepancies for further\n",
            "refinement of the system. This involved reviewing both the code’s logic and execution results\n",
            "to pinpoint areas needing improvement.\n",
            "6\n",
            "Algorithm 1 Autonomous Code Generation Process with AI Based Agents\n",
            "Require: High-level project description\n",
            "Ensure: Finalized code for each module, saved to separate files\n",
            "1: Initialize OpenAI API:\n",
            "2: Set the OPENAI API key with the appropriate API key.\n",
            "3: Define the model (e.g., ”gpt-4”).\n",
            "4: Get Module Descriptions:\n",
            "5: Read the project description from a file.\n",
            "6: Prepare the Manager agent prompt by reading Manager agent.txt and replacing\n",
            "placeholders.\n",
            "7: Initialize conversation history with the prepared prompt.\n",
            "8: Call the OpenAI API with the Manager agent prompt.\n",
            "9: Parse the response to extract and return the module descriptions.\n",
            "10: Get Pair Programmers Code:\n",
            "11: for each module description do\n",
            "12:\n",
            "Prepare prompts for Dev 1 agent and Dev 2 agent by reading respective files and\n",
            "replacing placeholders.\n",
            "13:\n",
            "Initialize Dev 1conversation history and Dev 2 agent conversation history\n",
            "with respective prompts.\n",
            "14:\n",
            "Conduct 3 rounds of interaction between Dev 1 and Dev 2 agents:\n",
            "15:\n",
            "for each round do\n",
            "16:\n",
            "Dev 1 agent sends a message, and Dev 2 agent responds.\n",
            "17:\n",
            "Dev 2 agent sends a message, and Dev 1 agent responds.\n",
            "18:\n",
            "Update the conversation histories accordingly.\n",
            "19:\n",
            "end for\n",
            "20:\n",
            "Extract the final code from the last responses containing Python code.\n",
            "21:\n",
            "Return the final code.\n",
            "22: end for\n",
            "23: Verify Code:\n",
            "24: Prepare the Verification agent prompt by reading verification agent.txt and re-\n",
            "placing placeholders.\n",
            "25: Initialize conversation history with the prepared prompt.\n",
            "26: Call the OpenAI API with the Verification agent prompt.\n",
            "27: Parse the response to extract and return the review.\n",
            "28: Finalize Code:\n",
            "29: Prepare prompts for Finalized 1 agent by reading respective files and replacing place-\n",
            "holders.\n",
            "30: Initialize Finalized 1 agent conversation history and Finalized 2 agent\n",
            "conversation history with respective prompts.\n",
            "31: Conduct 3 rounds of interaction between Dev 1 agent and Dev 2 agent:\n",
            "32: for each round do\n",
            "33:\n",
            "Finalized 1 agent sends a message, and Finalized 2 agent responds.\n",
            "34:\n",
            "Finalized 2 agent sends a message, and Finalized 1 agent responds.\n",
            "35:\n",
            "Update the conversation histories accordingly.\n",
            "36: end for\n",
            "37: Extract the final code from the last responses containing Python code.\n",
            "38: Return the final code.\n",
            "39: Main Execution:\n",
            "40: Read the project description from a file.\n",
            "41: Get module descriptions by calling get module descriptions.\n",
            "42: Initialize accumulated code as an empty string.\n",
            "43: for module in module descriptions do\n",
            "44:\n",
            "Get module code by calling get pair programmers code.\n",
            "45:\n",
            "Optionally, get review by calling get verification review\n",
            "46:\n",
            "Save the module code to a file named after the module.\n",
            "47: end for\n",
            "7\n",
            "2.3.1. HumanEval Benchmark\n",
            "Chen et al. [20] proposed the HumanEval benchmark to examine the ability of proposed\n",
            "systems to generate code in response to human-written programming tasks. The HumanEval\n",
            "benchmark consists of 164 programming problems, each with a problem description, a set of\n",
            "unit tests, and a function signature [24]. In our evaluation methodology, we used the pass@k\n",
            "metric, an established approach for testing the performance of systems in code generation tasks.\n",
            "This approach includes evaluating the responses generated by our proposed system. When\n",
            "tested against the HumanEval benchmark, a code generation system receives the function\n",
            "signatures and problem statements, and is tasked with generating a complete function body\n",
            "for each problem. A key aspect of this benchmark is that the generated code is executed against\n",
            "the provided unit tests. The results of these tests determine the accuracy and functionality of\n",
            "the generated code. This evaluation process helps assess the system’s ability to handle diverse\n",
            "programming challenges.\n",
            "def\n",
            "pass at k (n ,\n",
            "c , k ) :\n",
            "”””\n",
            ": param n :\n",
            "t o t a l\n",
            "number of\n",
            "samples\n",
            ": param c : number of\n",
            "correct\n",
            "samples\n",
            ": param k :\n",
            "k in pass@$k$\n",
            "”””\n",
            "i f n −c < k :\n",
            "return 1.0\n",
            "return 1.0 −np . prod (1.0 −k / np . arange (n −c + 1 , n + 1))\n",
            "Figure 2: A numerically stable script for calculating an unbiased estimate of pass@k.\n",
            "The pass@k metric is a performance measure used to evaluate the functional correctness\n",
            "of code generated by LLMs [20]. The pass@k metric quantifies the probability that at least\n",
            "one out of k independently generated samples from a model passes all unit tests for a given\n",
            "problem, where k code samples are generated per problem, a problem is considered solved if\n",
            "any sample passes the unit tests, and the total fraction of problems solved is reported. n is\n",
            "the total number of possible answers and c is the number of correct answers. For instance,\n",
            "CodePori generates responses for a diverse set of inputs.\n",
            "For each input, we check if the\n",
            "top (first) response from our algorithm is correct or satisfactory, recording this as a binary\n",
            "outcome: 1 (success) if the first response is correct, and 0 (failure) otherwise. The Pass@1\n",
            "score is calculated as the number of successful first responses divided by the total number of\n",
            "inputs. A higher Pass@1 score indicates better performance, ensuring that the generated code\n",
            "is correct and functional. Below, we present the formula that defines the pass@k metric.\n",
            "pass@k := EProblems\n",
            "\"\n",
            "1 −\n",
            "\u0000n−c\n",
            "k\n",
            "\u0001\n",
            "\u0000n\n",
            "k\n",
            "\u0001\n",
            "#\n",
            "Where: Eproblems denotes the expected value over all problems. n is the total number of\n",
            "possible answers and c is the number of correct answers. k is the rank position up to which we\n",
            "consider the answer as passed.\n",
            "In Figure 2, we present a numerically stable implementation using numpy, which streamlines\n",
            "the mathematical expression and efficiently computes the product iteratively, minimizing the\n",
            "risk of numerical errors and ensuring accurate results even with large datasets or challenging\n",
            "parameters. It might be tempting to approximate pass@k with 1−(1−ˆp)k, where ˆp represents\n",
            "the empirical estimate of pass@1.\n",
            "We compare our proposed system with recent developed systems for code generation field,\n",
            "including MetaGPT [19], ChatDev [21], AlphaCode [22], Incoder [23], CodeGeeX [24], Codex\n",
            "[20] and general domain LLMs such as PaLMCoder [25].\n",
            "8\n",
            "Table 3: Input Descriptions\n",
            "ID\n",
            "Input Description\n",
            "D1\n",
            "Create a Python-based facial recognition system for automated attendance tracking, with a Tkinter UI and CSV data logging.\n",
            "D2\n",
            "Create a face mask detection system using computer vision techniques to accurately identify whether individuals are wearing\n",
            "face masks in real-time video streams.\n",
            "D3\n",
            "Build a human temperature screening system using infrared thermal imaging to accurately measure and monitor individuals’\n",
            "body temperatures in real-time.\n",
            "D4\n",
            "Create a chat-based system, which name PekkaGPT, that uses advanced natural language processing to provide interactive\n",
            "and intelligent conversations.\n",
            "D5\n",
            "Build a website search strategy tool to optimize and enhance search engine performance and user query results.\n",
            "D6\n",
            "Create a qualitative data analysis tool to process and derive insights from large datasets efficiently and accurately.\n",
            "D7\n",
            "Build a presidential debate tool that supports nine candidates and operates in the Finnish language, allowing users to ask\n",
            "questions in Finnish and receive candidate responses accordingly.\n",
            "D8\n",
            "Create a restaurant chain system using LangChain technology, utilizing a provided dataset in PDF format, with the goal\n",
            "of automating the restaurant chain operations.\n",
            "D9\n",
            "Create a tic-tac-toe game that allows two players to compete in a simple and interactive grid-based environment,\n",
            "with turn-based gameplay and win/loss detection.\n",
            "D10\n",
            "Develop a snake game with a graphical user interface (GUI) that features interactive gameplay, score tracking, and increasing\n",
            "difficulty as the snake grows longer.\n",
            "D11\n",
            "Build a search strategy tool for the industry dataset to optimize and enhance data retrieval, enabling efficient and accurate\n",
            "querying and analysis.\n",
            "D12\n",
            "Build a chat-based application that allows users to communicate in real-time with features such as message notifications, user\n",
            "authentication, and a user-friendly interface.\n",
            "D13\n",
            "A simple Python program for creating and using flashcards for study, allowing users to test their knowledge on various\n",
            "subjects.\n",
            "D14\n",
            "Build a real-time weather app that provides up-to-date weather information, including temperature, humidity, and forecasts, with\n",
            "a user-friendly interface and location-based services.\n",
            "D15\n",
            "Create a fitness tracker app that monitors and records physical activities, tracks progress over time, and provides personalized\n",
            "health and fitness recommendations with an intuitive user interface.\n",
            "D16\n",
            "A Python-based Pomodoro timer that helps users apply the Pomodoro technique (25 minutes of work followed by a 5-\n",
            "minute break) to boost productivity.\n",
            "D17\n",
            "A short, interactive, text-based adventure game written in Python, where players make choices that influence the outcome of the\n",
            "story.\n",
            "D18\n",
            "Develop a personal finance tracker app that enables users to monitor their income, expenses, and savings, set budget goals, and\n",
            "generate financial reports, all within a user-friendly interface.\n",
            "D19\n",
            "Develop a short e-commerce website that allows users to browse products, add items to a shopping cart, and securely check\n",
            "out, featuring user authentication, product search, and order management functionalities.\n",
            "D20\n",
            "Create a recipe finder app that allows users to search for recipes based on ingredients, cuisine, or dietary preferences, featuring\n",
            "user reviews, step-by-step instructions, and the ability to save favorite recipes.\n",
            "2.3.2. Manual Evaluation\n",
            "The purpose of manual evaluation is to assess the accuracy and performance of generated\n",
            "code. Manual evaluation involves providing the system with various project descriptions and\n",
            "then manually reviewing the generated code for accuracy and performance.\n",
            "Selection of Input Descriptions. To conduct a comprehensive evaluation, we selected 20 dif-\n",
            "ferent types of Python project descriptions, as shown in Table 3.\n",
            "These descriptions were\n",
            "carefully chosen by six AI-experienced developers with diverse industrial backgrounds. These\n",
            "descriptions were selected to cover different domains to test the system’s ability to handle\n",
            "different types of projects. This includes machine learning, data analysis, mobile apps, web\n",
            "development, and automation scripts. The main purpose is to make sure that the system is\n",
            "tested across a broad spectrum of coding tasks.\n",
            "The project descriptions start from simple tasks, such as “Python code for Tic-Tac-Toe\n",
            "game”, to more complex projects including multi-step processes and advanced algorithms such\n",
            "as “restaurant chain system using lang-chain technology”. Each project description included\n",
            "specific functional requirements and constraints. These were detailed to provide clear guidance\n",
            "for the code generation process, ensuring that the system has sufficient information to generate\n",
            "accurate and relevant code.\n",
            "Manual Execution of CodePori. After selecting the project descriptions, the next step was\n",
            "to manually run the CodePori system with each of the 20 input descriptions. The system\n",
            "processed the descriptions and generated corresponding code for each project. The generated\n",
            "code was then reviewed by first author for initial assessment and prepared for further analysis.\n",
            "The generated code was subjected to an initial review to identify any immediate errors. This\n",
            "step ensured that the code was ready for the detailed analysis phase. During the initial review,\n",
            "we checked the code for basic syntax errors, missing dependencies, and compatibility with\n",
            "9\n",
            "the specified programming environment. If minor adjustments were necessary to correct the\n",
            "code, these were documented carefully. The manually adjusted code was then re-evaluated to\n",
            "ensure it complete the initial requirements. A uniform execution environment was established\n",
            "to ensure consistency in testing. This included setting up the necessary development tools,\n",
            "libraries, and frameworks required to run the generated code. The environment was configured\n",
            "to align with specifications provided in the project descriptions, ensuring that the generated\n",
            "code tested under authentic conditions.\n",
            "For all 20 projects, a replication package was created [26].\n",
            "This package included the\n",
            "generated code, any required changes, each project output, and detailed instructions on how\n",
            "to execute the code. The replication package was organized systematically with clear labeling\n",
            "to ensure transparency and accessibility during the analysis phase.\n",
            "The main goal of the\n",
            "replication package that served as a comprehensive record of the generated code, allowing for\n",
            "easy verification and re-evaluation.\n",
            "Code Analysis. The code analysis was conducted to evaluate the accuracy and functionality\n",
            "of the generated code. The main goal was to determine whether the code generated by the\n",
            "automated system executed successfully and meet the project requirements. The following\n",
            "methodology was used for the analysis:\n",
            "We executed generated code to verify its functionality. If the code executed correctly and\n",
            "produced the expected results, it was marked as “pass”. If the code did not run initially, and\n",
            "needed minor changes to correct syntax errors, missing dependencies, or other minor issues and\n",
            "after these adjustments, the code ran successfully, it was still marked as “pass”. If the code\n",
            "failed to run even after making reasonable adjustments, it was marked as “fail”. The failure\n",
            "was documented with details on the issues encountered and the attempted fixes. The accuracy\n",
            "of the system was calculated using the formula:\n",
            "Accuracy =\n",
            "\u0012 Number of Passes\n",
            "Total Descriptions\n",
            "\u0013\n",
            "× 100\n",
            "For example, in this paper, 17 out of 20 descriptions resulted in correct and functional code,\n",
            "the accuracy was calculated as follows:\n",
            "Accuracy =\n",
            "\u001217\n",
            "20\n",
            "\u0013\n",
            "× 100 = 85%\n",
            "A high pass rate indicates that the system is effective in generating functional code based\n",
            "on the given descriptions. A pass rate of 85% (i.e., 17 out of 20 descriptions passing) indicates\n",
            "a high-level of accuracy and reliability in the system’s code generation capabilities. On the\n",
            "other hand, the code that failed was analyzed in detail to identify common issues that will be\n",
            "addressed in future system. This analysis helps in understanding the limitations of the current\n",
            "system and provides insights for improvements.\n",
            "3. Results\n",
            "This section reports the study results of the proposed CodePori system.\n",
            "Our findings\n",
            "indicate that the proposed system is capable of generating accurate code for large and complex\n",
            "software projects within a few minutes. Below, we present the results of our proposed system\n",
            "in Section 3.1. In this section, we also provide a demonstration of the software developed by\n",
            "the proposed system. We also conducted an extensive evaluation of CodePori against existing\n",
            "systems and report the results in Section 3.2. We used HumanEval benchmarks and manually\n",
            "tested our proposed system against existing systems.\n",
            "10\n",
            "Figure 3: Software developed by CodePori\n",
            "3.1. CodePori: LLM-Based Multi-Agent System (RQ1)\n",
            "The implementation of CodePori, an LLM-based multi-agent system, represents a step\n",
            "forward in automating complex software development tasks. In this system, we assigned each\n",
            "agent to handle various aspects of software development, including code development, code\n",
            "review, code verification, and test engineering. This specialization allowed for a more focused\n",
            "and efficient approach to each task. These agents collaborate to ensure the accurate execution\n",
            "of each phase of the software development process.\n",
            "Our findings indicate that CodePori is able to handle large coding tasks. For instance,\n",
            "existing systems are capable of handling smaller projects typically ranging from 100 to 200\n",
            "lines of code.\n",
            "However, when the requirements scale up to projects demanding over 1000\n",
            "lines of code, these systems often fail [19]. In contrast, CodePori is capable of managing such\n",
            "extensive coding projects. CodePori completed the entire software development cycle for large\n",
            "projects in less than twenty minutes. To verify these findings, we provided the demo in Figure\n",
            "3 for large and complex projects to show the capability and code scalability of CodePori.\n",
            "3.1.1. Software Developed by CodePori\n",
            "Figure 3 presents a detailed representation of the software systems developed by our mod-\n",
            "eling techniques. It begins with a user’s input command (as shown in Table 3) and ends with\n",
            "software designed according to the user’s specifications. Upon receiving an instruction from\n",
            "the user, multi-agents collaborate with each other to fulfill the task. It became evident that\n",
            "significant modifications and improvements were necessary to refine the generated code to a\n",
            "level where it could produce accurate results. For example, it was necessary to install the\n",
            "required libraries, update the file paths for the CSV or other required files, and, if needed,\n",
            "load the appropriate API or pre-trained model. In Section 3.2.2, we discussed the necessary\n",
            "changes made to the generated code to obtain the results. Figure 3 demonstrates the outcomes\n",
            "of various software applications developed by CodePori, including a face recognition system, a\n",
            "chat-bot system, a face mask detection tool, a snake game, a calculator, a Tic-Tac-Toe game,\n",
            "a data analysis system, and a presidential debate system.\n",
            "3.2. Performance Evaluations’ Results (RQ2)\n",
            "To assess the performance of CodePori in comparison with existing systems, we conducted\n",
            "extensive evaluations using the HumanEval benchmark and manual evaluation. These eval-\n",
            "11\n",
            "0\n",
            "10\n",
            "20\n",
            "30\n",
            "40\n",
            "50\n",
            "60\n",
            "70\n",
            "80\n",
            "90\n",
            "100\n",
            "AlphaCode\n",
            "Incoder\n",
            "CodeGeeX\n",
            "PalmCoder\n",
            "Codex\n",
            "ChatDev\n",
            "MetaGPT\n",
            "CodePori\n",
            "Pass@1\n",
            "Pass@1\n",
            "Figure 4: HumanEval benchmark results\n",
            "uations were structured to provide a comparative analysis against existing systems including\n",
            "MetaGPT [19], ChatDev [21], AlphaCode [22], Incoder [23], CodeGeeX [24], Codex [20] and\n",
            "general domain LLMs such as PaLMCoder [25]. Our findings indicated that CodePori out-\n",
            "performed the existing solutions in code accuracy and efficiency. The experimental results of\n",
            "HumanEval benchmarks are summarized in Figure 4. The proposed system has the capability\n",
            "to understand complex prompts and translate them into functional code, reducing the inci-\n",
            "dence of errors and bugs. Additionally, the results show a reduction in the time required to\n",
            "complete software development tasks compared to other systems. As illustrated in Figure 4,\n",
            "CodePori surpasses its counterparts in HumanEval benchmarks. For pass@1 metric, CodePori\n",
            "achieves 89% in terms of code accuracy. For the manual evaluation, we provided 20 input\n",
            "descriptions and obtained 17 accurate results, resulting in a success rate of 85%.\n",
            "3.2.1. HumanEval\n",
            "The evaluation of CodePori against existing systems using the HumanEval benchmark\n",
            "demonstrates the capabilities of CodePori. The benchmark, well-known for testing of code\n",
            "generation systems, provides a comparison between CodePori and existing systems. As indi-\n",
            "cated in Figure 4, CodePori achieved 89% for pass@1 metric, highlighting the capability of\n",
            "CodePori to understand requirements and generate code based on them.\n",
            "In contrast, as shown in Figure 4, existing systems like AlphaCode and InCoder achieved\n",
            "lower scores, with scores of 18% and 17% respectively for pass@1. PaLMCoder and CodeX\n",
            "achieved scores of 37% and 48% respectively for pass@1. These results highlight that CodePori\n",
            "presents a step forward in automated code generation.\n",
            "3.2.2. Manual Evaluation\n",
            "Figure 3 shows CodePori’s capabilities in automated software development. The evaluation\n",
            "of the CodePori system’s performance was based on 20 project descriptions. Out of the 20\n",
            "project descriptions, the generated code successfully executed for 17 descriptions after minor\n",
            "adjustments.\n",
            "This corresponds to a pass rate of 85%, indicating that CodePori was able\n",
            "to produce accurate code for the vast majority of the given inputs. The 3 descriptions for\n",
            "which the generated code failed to execute, even after adjustments, were analyzed in detail.\n",
            "The failures were determined to primarily result from complex logical errors and incomplete\n",
            "12\n",
            "Table 4: Result produced by CodePori\n",
            "S.No\n",
            "Input ID\n",
            "Line of\n",
            "Codes\n",
            "Mins\n",
            "Modules\n",
            "OpenAI Bill\n",
            "Output\n",
            "01\n",
            "D1\n",
            "477\n",
            "20\n",
            "5\n",
            "1.45$\n",
            "Face register, face recognize, and manually attendance\n",
            "02\n",
            "D2\n",
            "399\n",
            "17\n",
            "4\n",
            "1.02$\n",
            "Mask detection on face\n",
            "03\n",
            "D3\n",
            "444\n",
            "18\n",
            "2\n",
            "1.20$\n",
            "Predict human temperature by screening face\n",
            "04\n",
            "D4\n",
            "580\n",
            "27\n",
            "4\n",
            "1.80$\n",
            "Chat-based application\n",
            "05\n",
            "D5\n",
            "412\n",
            "20\n",
            "3\n",
            "1.30$\n",
            "Search and extract data from given input website\n",
            "06\n",
            "D6\n",
            "789\n",
            "35\n",
            "7\n",
            "2.10$\n",
            "Perform analysis on given input dataset\n",
            "07\n",
            "D7\n",
            "1180\n",
            "40\n",
            "9\n",
            "2.56$\n",
            "Presidential debate system\n",
            "08\n",
            "D8\n",
            "417\n",
            "19\n",
            "3\n",
            "1.40$\n",
            "Automate the process of restaurant chain\n",
            "09\n",
            "D9\n",
            "101\n",
            "5\n",
            "1\n",
            "0.40$\n",
            "Tic-Tac-Toe game with GUI\n",
            "10\n",
            "D10\n",
            "115\n",
            "4\n",
            "1\n",
            "0.50$\n",
            "Snake game with restart button\n",
            "11\n",
            "D11\n",
            "311\n",
            "14\n",
            "4\n",
            "1.10$\n",
            "Search strategy platform for a industry\n",
            "12\n",
            "D12\n",
            "175\n",
            "7\n",
            "2\n",
            "1.20$\n",
            "Messaging chat based app\n",
            "13\n",
            "D13\n",
            "94\n",
            "4\n",
            "1\n",
            "0.20$\n",
            "Create a flashcard for study purpose\n",
            "14\n",
            "D14\n",
            "119\n",
            "5\n",
            "1\n",
            "0.30$\n",
            "Real-time weather forecasting app\n",
            "15\n",
            "D15\n",
            "107\n",
            "4\n",
            "1\n",
            "0.20$\n",
            "Real-time fitness tracker app\n",
            "16\n",
            "D16\n",
            "189\n",
            "9\n",
            "2\n",
            "0.80$\n",
            "Developed virtual app\n",
            "17\n",
            "D17\n",
            "149\n",
            "6\n",
            "2\n",
            "0.67$\n",
            "Blog platform\n",
            "18\n",
            "D18\n",
            "134\n",
            "5\n",
            "2\n",
            "0.55$\n",
            "Finance app\n",
            "19\n",
            "D19\n",
            "151\n",
            "8\n",
            "2\n",
            "0.69$\n",
            "Small E-commerce website\n",
            "20\n",
            "D20\n",
            "133\n",
            "4\n",
            "2\n",
            "0.53$\n",
            "Recipe search app\n",
            "implementation of the specified requirements. This highlights areas where the system may\n",
            "need further training or refinement.\n",
            "Modifications in Generated Code. In this section, we discuss the modifications made to projects.\n",
            "During the manual execution of the generated code, certain changes were necessary to ensure\n",
            "correct functionality. We provide the replication package of the generated code along with a\n",
            "description of the modifications made to achieve accurate results [26]. Below, we discuss the\n",
            "specific modifications made to ensure the execution of three selected projects. Modifications\n",
            "for the remaining projects are provided in the replication package.\n",
            "For project D1, we installed the “haarcascade frontalface default.xml” file from GitHub and\n",
            "stored it in the same directory as the D1 project. This file is utilized for detecting frontal faces.\n",
            "Additionally, we created two CSV files in the same project directory, named “attendance.csv”\n",
            "and “registration.csv”. These CSV files are used to store the respective records. It is also\n",
            "necessary to install all the libraries mentioned in the code. In project D2, we downloaded\n",
            "the publicly available pre-trained model named “mask.detector.model”. We then updated the\n",
            "path in the code and made the necessary changes.\n",
            "Additionally, we installed the required\n",
            "libraries, including Numpy, OpenCV, imutils, and Keras model. In project D3, we added You\n",
            "Only Look Once (YOLO) model in the code to detect objects, especially in detecting people\n",
            "and non-people. While the calculation of body temperature, we deployed thermal images to\n",
            "detect temperature. For project D5, we used the wiki.eduu website as a dataset. We scraped\n",
            "the entire website’s data into PDF format, deployed the dataset into the same directory, and\n",
            "provided the path in the code. For large projects that consist of more than two modules,\n",
            "changes are always needed, as mentioned above. For small projects such as D9, D10, D13,\n",
            "D17, and D20, the code run directly without any changes. All modifications were documented,\n",
            "and the adjusted code was re-evaluated to ensure it met the initial requirements and executed\n",
            "successfully. D12, D15, and D19 projects failed to execute due to syntax and logical errors in\n",
            "the code.\n",
            "Overall Assessment. The CodePori demonstrated a high-level of accuracy and reliability in\n",
            "generating functional code for a diverse set of project descriptions. With a pass rate of 85%,\n",
            "CodePori shows significant potential for automating code generation tasks in software devel-\n",
            "opment. However, the identified areas for improvement suggest that further refinement and\n",
            "training of the system could enhance its performance, particularly in handling more complex\n",
            "project requirements. For instance, The system showed some weaknesses in handling complex\n",
            "logical conditions and edge cases, which contributed to the failures in 3 descriptions. While the\n",
            "13\n",
            "code was readable, the inclusion of more comprehensive comments and documentation would\n",
            "enhance understandability and maintainability.\n",
            "4. Discussion\n",
            "Recently, LLMs have experienced significant growth in the field of SE, transforming various\n",
            "aspects of the field. In this paper, we utilized an LLM-based agent to automate the code\n",
            "generation process. Our study explores the potential of these agents in software development.\n",
            "Our main goal is to automate the code generation process and reduce development time to\n",
            "assist practitioners and researchers. Below, we discuss the implications of the results, their\n",
            "challenges, and potential future works.\n",
            "4.1. LLM-Based Agents Code Generation (RQ1)\n",
            "In this study, we utilized six LLM-based agents in software development.\n",
            "The results\n",
            "demonstrated that these agents were able to generate accurate and functional code for a vari-\n",
            "ety of programming problems. The generated code was evaluated based on its accuracy and\n",
            "efficiency to the given project description.\n",
            "LLM-Based Agents. The implementation of LLM-based agents in software development has\n",
            "various implications. These agents can reduce development time and assist practitioners in\n",
            "their software development tasks, enabling developers to focus on more complex and creative\n",
            "aspects of their projects. This can lead to increased productivity and efficiency within develop-\n",
            "ment teams. Additionally, the ability to autonomously generate code can lead to cost savings,\n",
            "as projects are completed more efficiently and with fewer resources [19]. This development\n",
            "also contributes to academia by enhancing the capabilities and efficiency of AI-driven software\n",
            "development. For instance, the ethical and social implications of deploying LLM-based agents\n",
            "in software development present important research opportunities, such as studying the im-\n",
            "pact on the workforce, data privacy concerns, and potential biases in AI-generated code [30].\n",
            "However, certain challenges and risks must be considered by researchers and practitioners.\n",
            "Firstly, the capabilities of LLM-based agents in software development might be overesti-\n",
            "mated. As Qian et al. [21] mentioned, LLM-based agents improve development quality but\n",
            "often implement basic logic, leading to low information density. Without clear, detailed project\n",
            "input descriptions, agents struggle to understand task requirements. For instance, when pro-\n",
            "vided with general guidelines for developing a Tic-Tac-Toe game, agents often produce basic\n",
            "implementations. Similarly, in information management systems, these agents might utilize\n",
            "static key-value pairs instead of integrating with external databases. Consequently, it is im-\n",
            "portant to offer more detailed and precise software requirements or input descriptions. At\n",
            "present, these AI technologies are more appropriate for prototype systems rather than real-\n",
            "world applications.\n",
            "Secondly, automating the evaluation of general-purpose software is significantly more com-\n",
            "plex than traditional function-level code generation. Although some efforts, like those focusing\n",
            "on Human Revision Cost [19], have been made, manual verification remains impractical for\n",
            "large datasets.\n",
            "Our paper focuses on consistency and overall quality.\n",
            "However, future re-\n",
            "search should also consider additional factors such as functionality, robustness, safety, and\n",
            "user-friendliness.\n",
            "LLM-Based Generated Code. Autonomous code generation has gained significant attention\n",
            "due to its potential to reduce development costs. Today, LLMs are increasingly integral to\n",
            "software development, particularly in automating code generation. LLM-generated code makes\n",
            "programming more accessible to non-experts, enabling those with limited coding knowledge\n",
            "to contribute to software projects.\n",
            "This integration introduces a new era of efficiency and\n",
            "innovation in software development, while also introducing new challenges that need to be\n",
            "managed carefully. According to Dou et.al. [31], this technology needs support to produce\n",
            "accurate code and raises important ethical and practical considerations, such as ensuring the\n",
            "14\n",
            "security and correctness of generated code and addressing potential biases in the training\n",
            "data. Another challenge is inefficiency in handling out-of-memory errors. Larger code outputs\n",
            "can occasionally lead to memory or data management issues, resulting in the omission of\n",
            "important sections of code and producing incorrect outputs. To address these memory issues,\n",
            "we segmented the code into multiple modules, effectively resolving memory constraints and\n",
            "ensuring the integrity of the generated code.\n",
            "We argue that there is a need to extend the system’s capabilities to understand a wider\n",
            "range of programming languages and frameworks, which would make it more capable and useful\n",
            "in diverse development contexts. Additionally, integrating the proposed system with existing\n",
            "Integrated Development Environments (IDEs), version control systems, and other software\n",
            "development tools would make it a more consistent part of the software development ecosystem.\n",
            "Finally, we believe that enhancing the proposed system’s learning capabilities based on user\n",
            "input and customizing its outputs according to specific user preferences or project requirements\n",
            "could further improve its utility.\n",
            "4.2. Evaluation Framework (RQ2)\n",
            "HumanEval is widely used benchmarks in code generation with high-quality problem de-\n",
            "scriptions, diverse unit tests, and formatted input/output. The successful evaluation of our\n",
            "CodePori system for code generation confirms its capability in understanding and translating\n",
            "natural language descriptions into functional code. However, HumanEval benchmark typically\n",
            "covers 164 programming tasks, which is a limited set of projects and does not represent the\n",
            "full range of real-world programming challenges [32]. Also Dai et al. [33] mentioned, the pro-\n",
            "gramming tasks contained in the HumanEval datasets are designed for small projects. This\n",
            "represents a notable gap when it comes to evaluating systems intended to generate code for\n",
            "large and complex projects. We argue that including a broader range of tasks from various do-\n",
            "mains and difficulty levels, HumanEval can better simulate real-world programming challenges\n",
            "and provide a more comprehensive evaluation of LLM models’ capabilities.\n",
            "Finally, we acknowledge the limitations of using a single evaluation benchmark in testing\n",
            "the performance of LLMs in code generation tasks. Using only one evaluation metric may not\n",
            "fully capture the system’s capabilities and limitations. Using multiple evaluation metrics to\n",
            "evaluate the system could produce implications that a single metric might overlook.\n",
            "5. Related Work\n",
            "In this section, we present the related studies with a focus on existing research. Section\n",
            "5.1 provides an overview of studies concerning generative AI and its application across various\n",
            "fields, including NLP and GPT models. Section 5.2 provides an overview of studies concerning\n",
            "LLMs and SE. Finally, section 5.3 reviews works that have utilized LLMs for code generation.\n",
            "5.1. Generative Artificial Intelligence\n",
            "Generative AI specifically generate novel content similar to human-made creations [34],\n",
            "[35]. There have been significant advancements in the field of Generative AI recently [36]. In\n",
            "recent years, generative AI has found applications across multiple domains, including com-\n",
            "puter vision, NLP, and the creation of videos and images [37], [38]. In the domain of NLP,\n",
            "generative AI is frequently employed for a range of activities, such as generating text, devel-\n",
            "oping dialogue systems, translating languages, and generating code. According to Aydin et\n",
            "al. [39], researchers have utilized generative AI models to enhance different tasks in NLP.\n",
            "Techniques like Generative Adversarial Networks (GANs) and autoregressive language models,\n",
            "including GPT, which fall under the umbrella of generative AI, have been used for activities\n",
            "such as creating dialogue systems, translating languages, and generating text. The founda-\n",
            "tion of the GPT model can be traced back to the introduction of the transformer architecture\n",
            "proposed by Vaswani et al. [40]. This groundbreaking architecture transformed the NLP field\n",
            "with the introduction of the self-attention mechanism. The development of the self-attention\n",
            "15\n",
            "mechanism has revolutionized the NLP field [41]. It enabled the model to understand the con-\n",
            "textual relationships between words, regardless of their placement in a sequence [42], [43]. In\n",
            "2018, OpenAI presented the GPT-1 model, which showed the remarkable capabilities of large-\n",
            "scale language models in text-generation tasks [2]. LLMs consist of billions of parameters, are\n",
            "trained on large datasets, and have outstanding performance in processing tasks involving both\n",
            "natural and programming languages [44], [45]. LLMs designed for processing and generating\n",
            "human-like text, and GPT is a well-known example within this category [46]. GPT models\n",
            "are known for their capability to handle a wide range of language tasks, including translation,\n",
            "summarization, question-answering, and creative writing, all without requiring training specific\n",
            "to each task [2]. Advances in GPT models have increased their effectiveness for different tasks\n",
            "and their usability in various industries. Many researchers, along with OpenAI, have played an\n",
            "important role in improving the performance of GPT models through multiple methods and\n",
            "strategies [2, 3, 4, 5].\n",
            "5.2. Large Language Models in Software Engineering\n",
            "In recent years, LLMs have demonstrated significant growth in various SE applications [47].\n",
            "Nowadays, GPT models have shown potential across various SE applications [47]. These models\n",
            "have been trained on large code repositories, enabling them to generate code or entire programs\n",
            "based on input description [48]. According to Treude et al. [1], the GPT model and SE are\n",
            "interconnected by applying NLP techniques to different tasks within the software development\n",
            "lifecycle. GPT’s language generation capabilities provide valuable support and improvements\n",
            "to SE processes [49], [50]. LLMs have shown significant advantages over traditional methods,\n",
            "such as domain-specific languages, basic natural language models, and probabilistic grammars.\n",
            "Nowadays, LLMs have been applied to different field of SE. These include data analysis [13],\n",
            "software development [14], text classification [12], code search [15], automated program repair\n",
            "[22], unit test case generation [51], etc. Ma et al. [52], Nascimento et al. [53], and Rasheed\n",
            "et al. [54] conducted comprehensive empirical studies to investigate the capabilities of GPT\n",
            "model for various SE tasks.\n",
            "Hou et al. [55] conducted a systematic literature review on LLM for SE (LLM4SE), mainly\n",
            "focusing on optimizing processes and outcomes through LLMs in SE. In this study, they ana-\n",
            "lyzed 229 research papers published between 2017 and 2023. The research mainly focuses on\n",
            "four main areas: (i) Examining various LLM types used in SE tasks, detailing their unique\n",
            "features and applications. (ii) Analyzing the techniques for data gathering, pre-processing,\n",
            "and implementation. (iii) Exploring the approaches for optimizing and measuring LLM per-\n",
            "formance in SE. (iv) Reviewing the specific SE tasks where LLMs have shown success, high-\n",
            "lighting their contributions to the field. The surveys discuss the most advanced technologies\n",
            "and emerging trends, determine the gaps in current research, identify the key challenges in\n",
            "utilizing LLMs for SE, and suggest several potential research directions for LLM4SE.\n",
            "Zheng et al. [56] also conducted an comprehensive review of research and products that\n",
            "integrate LLMs with SE. In this paper, they investigated the potential of LLMs to improve\n",
            "current SE tasks and categorized these tasks into seven types. For each types, they presented\n",
            "application examples of LLMs, highlighting their strengths and limitations to facilitate re-\n",
            "searchers in recognizing and addressing potential challenges in applying LLMs to SE tasks.\n",
            "They also identified several future directions worth exploring: (1) the current performance\n",
            "of LLMs on certain SE tasks is inconsistent; (2) most evaluations are based on general large\n",
            "models, such as ChatGPT, and lack detailed assessments of code-centric models like Codex;\n",
            "(3) the necessity of customizing large models for specific SE tasks remains an open question.\n",
            "Zheng et al. [57] examined the effectiveness and significance of LLMs in the context of SE.\n",
            "They reviewed and evaluated 134 studies focused on code LLMs, highlighting the connections\n",
            "between code-specific LLMs and general-purpose LLMs. Furthermore, they conducted an in-\n",
            "depth analysis of how both general and code LLMs perform in various SE tasks, providing a\n",
            "detailed assessment of their capabilities across different sub-tasks.\n",
            "Shin et al. [58] explored the capabilities of GPT-4, by integrating various prompting tech-\n",
            "niques (such as basic prompts, context learning, and task-specific prompts) to assess its per-\n",
            "16\n",
            "Table 5: Large pre-trained language models for code automation\n",
            "S No\n",
            "Papers\n",
            "Parameters\n",
            "Language\n",
            "Size\n",
            "Benchmark\n",
            "Reference\n",
            "01\n",
            "Codex\n",
            "12B\n",
            "Python\n",
            "Code: 159GB\n",
            "HumanEval, APPS\n",
            "Chen et al. [20]\n",
            "02\n",
            "AlphaCode\n",
            "41B\n",
            "12 Langs\n",
            "Code: 715.1GB\n",
            "HumanEval, APPS\n",
            "Code Contest\n",
            "Li et al. [22]\n",
            "03\n",
            "PaLM-Coder\n",
            "8B\n",
            "Multiple\n",
            "Text: 741 B Tokens\n",
            "Code: 39GB\n",
            "HumanEval, MBPP\n",
            "TransCoder, DeepFix\n",
            "Chowdhery et al. [25]\n",
            "04\n",
            "PolyCoder\n",
            "2.7B\n",
            "12 Langs\n",
            "Code: 253.6GB\n",
            "HumanEval,\n",
            "Xu et al. [63]\n",
            "05\n",
            "GPT-Neo\n",
            "1.3B\n",
            "Multiple\n",
            "Text: 730GB\n",
            "Code: 96GB\n",
            "HumanEval,\n",
            "Black et al. [60]\n",
            "06\n",
            "GPT-NeoX\n",
            "20B\n",
            "Multiple\n",
            "Text: 730GB\n",
            "Code: 95GB\n",
            "HumanEval,\n",
            "Black et al. [66]\n",
            "07\n",
            "GPT-J\n",
            "6B\n",
            "Multiple\n",
            "Text: 730GB\n",
            "Code: 96GB\n",
            "HumanEval,\n",
            "Arora et al. [67]\n",
            "08\n",
            "CodeGen-Multi\n",
            "6.1B\n",
            "6 Langs\n",
            "Code: 150B Tokens\n",
            "Text: 355B Tokens\n",
            "HumanEval, MBPP\n",
            "Nijkamp et al. [64]\n",
            "09\n",
            "Incoder\n",
            "6.1B\n",
            "28 Langs\n",
            "Code: 159GB\n",
            "HumanEval, MBPP\n",
            "CodeXGLUE\n",
            "Fried et al. [23]\n",
            "10\n",
            "CodeGeeX\n",
            "13B\n",
            "23 Langs\n",
            "Code: 15B Tokens\n",
            "HumanEval, MBPP, XLCoST\n",
            "HumanEval-X, CodeXGLUE,\n",
            "Zheng et al. [24]\n",
            "11\n",
            "CodeGen-Mono\n",
            "6.1B\n",
            "Python\n",
            "Code: 150B tokens\n",
            "Text: 355B Tokens\n",
            "HumanEval, MTPB\n",
            "Nijkamp et al. [64]\n",
            "formance on three common SE tasks: code generation, code summarization, and code trans-\n",
            "lation. They compared GPT-4’s performance with 18 other fine-tuned LLMs. Additionally,\n",
            "they conducted user studies involving 27 academic participants and 10 industry professionals\n",
            "to evaluate the quality and utility of GPT-4’s responses and to observe how users generate\n",
            "dialogue prompts to guide the model. The findings revealed that GPT-4 outperformed the\n",
            "baseline in comment generation and C# to Java code translation but under-performed in code\n",
            "generation and Java to C# translation when using basic prompts.\n",
            "5.3. Code Generation with Large Language Models\n",
            "Nowadays, LLMs have shown impressive effectiveness across different SE domains. Many\n",
            "studies have investigated the potential of LLMs in facilitating automatic programming. For\n",
            "example, AlphaCode (Li et al. [22]) reportedly surpassed the performance of accurate code\n",
            "generation in actual programming contests, while Codex (Chen et al. [20]) is enhancing Copilot\n",
            "with the capability to offer instant coding recommendations.\n",
            "Wang et al.\n",
            "[59] introduced\n",
            "CodeT5, a cohesive pre-trained encoder-decoder Transformer model, using a unified approach\n",
            "to effortlessly facilitate both code comprehension and generation tasks, while enabling multi-\n",
            "task learning. Other open-source code generation models include GPTNeo (Black et al. [60]),\n",
            "GPT-J (Wang et al.\n",
            "[61]), CodeParrot (Tunstall et al.\n",
            "[62]), PolyCoder (Xu et al.\n",
            "[63]),\n",
            "CODEGEN (Nijkamp et al. [64]), INCODER (Fried et al. [23]), and (Rasheed et al. [14]).\n",
            "Chen et al.[65] leverage the Codex inference API from OpenAI, along with the two robust\n",
            "open-source models, CODEGEN and INCODER, for zero-shot code generation. As Qian et al.\n",
            "[21] introduce the ChatDev model, which utilizes LLMs across the entire software development\n",
            "cycle. This model demonstrates exceptional effectiveness in software creation, allowing for the\n",
            "completion of the full software development process in less than seven minutes at a cost of under\n",
            "one dollar. Zheng et al. [24] presented CodeGeeX, a 13-billion-parameter multilingual model\n",
            "designed for code generation. As of June 2022, CodeGeeX has been pre-trained on a massive\n",
            "dataset comprising 850 billion tokens from 23 different programming languages. The most\n",
            "recently, Hong et al. [19] introduced MetaGPT, an innovative meta-programming framework\n",
            "incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT\n",
            "outperforms all the existence approaches [19]. MetaGPT utilizes an assembly line paradigm to\n",
            "assign diverse roles to various agents, efficiently breaking down complex tasks into sub tasks\n",
            "involving many agents working together.\n",
            "However, MetaGPT, along with the previously discussed systems, still needs to demonstrate\n",
            "its effectiveness beyond controlled experiments and benchmarks. Moreover, the system’s abil-\n",
            "ity to efficiently manage tasks aligned with structured Standard Operating Procedures (SOPs)\n",
            "and predefined modular outputs remains questionable, often leading to inaccuracies in results.\n",
            "17\n",
            "This limitation underscores a vital area for improvement in adapting to the dynamic and un-\n",
            "structured nature of real-world challenges. In response, CodePori is designed to overcome these\n",
            "issues by integrating more adaptive algorithms. CodePori primarily focuses on an enhanced\n",
            "understanding of complex, real-world tasks and the capability to generate more accurate and\n",
            "efficient code for very large and complex projects.\n",
            "6. Threats to Validity\n",
            "In this section, we discuss some significant threats to the validity of our results.\n",
            "Dataset Validity.\n",
            "According to Wang et al. [68], dataset validity measures the accuracy and dependability\n",
            "of the information contained within a dataset. In this paper, the primary concern addresses\n",
            "potential issues associated with the benchmark dataset we used in our study. HumanEval\n",
            "benchmark contains 164 programming problems, which can thoroughly test the code genera-\n",
            "tion capabilities of the proposed system’s results [20]. However, this benchmark has several\n",
            "limitations. First, the problem descriptions lack sufficient context or detail about the problem\n",
            "domain. Furthermore, this lack of context makes it difficult for systems to generate accurate\n",
            "and relevant solutions. Additionally, some descriptions use images for explanations, but these\n",
            "are only indicated by the word “image”, preventing the LLM from fully understanding the\n",
            "problem [31]. To avoid these issues, we manually evaluate the CodePori system by manually\n",
            "giving input descriptions of various projects. The dataset of CodePori results for the given\n",
            "input descriptions is publicly available [26]. Another issue with the HumanEval benchmark is\n",
            "that it only includes Python language datasets; it does not provide a comprehensive assessment\n",
            "of a system’s ability to understand and generate code in a multilingual context [69], [70]. The\n",
            "final concern is that existing benchmarks may result in data leakage and out-of-memory prob-\n",
            "lems, as current LLMs are trained on large portions of GitHub, Stack Overflow, and various\n",
            "code libraries [71], [72], [73]. In the future, we aim to explore and assess the challenges that\n",
            "LLMs face in generating code for different programming languages.\n",
            "External Validity.\n",
            "According to Runeson et al. [74], external validity refers to the concept and applicability\n",
            "of the study results. Threats to external validity in our study involve the ability to generalize\n",
            "the findings from the CodePori system’s performance. To address these concerns, we evaluated\n",
            "our system using the HumanEval dataset, which encompasses a diverse set of coding tasks.\n",
            "Additionally, we manually evaluated CodePori’s performance by providing specific input de-\n",
            "scriptions and testing the generated code through execution. This dual approach ensures that\n",
            "our results are relevant and applicable to various real-world code generation scenarios, which\n",
            "enhances the reliability of our findings.\n",
            "Internal Validity.\n",
            "According to Runeson et al. [74], internal validity refers to how well a study minimizes bias\n",
            "in data collection. In our study, when evaluating the CodePori system, we ensured internal\n",
            "validity through several measures. First, we used manual evaluation of the generated code\n",
            "by providing specific input descriptions and executing the code to assess its functionality. To\n",
            "mitigate the risk of subjective biases and inconsistencies, we employed double-checking among\n",
            "different experts to reach a consensus on the performance of the generated code. This method\n",
            "reduces the likelihood of incorrect assessments due to individual variations in understanding\n",
            "or interpretation. Additionally, we maintained consistency in our evaluation process by stan-\n",
            "dardizing the settings and conditions under which the code was generated and tested, ensuring\n",
            "that our results accurately reflect the system’s capabilities. To ensure consistent responses\n",
            "from LLMs, we set the temperature to 0.1 and top-k to 1, following commonly used settings\n",
            "[75], [76].\n",
            "18\n",
            "7. Conclusions\n",
            "This study presents an LLM-based multi-agent system for code generation, CodePori. We\n",
            "evaluate CodePori using existing benchmarks, evaluating its performance against existing tools,\n",
            "as well as on its own. Based on the evaluation results, we argue that CodePori can reduce\n",
            "development time and cost, and advances code generation methodologies. CodePori is capable\n",
            "of autonomously developing large and complex software projects. Our evaluation of CodePori,\n",
            "using the HumanEval benchmark and a manual test with 20 input project descriptions, shows\n",
            "that CodePori achieves 89% code accuracy with the pass@1 metric and 85% code accuracy\n",
            "in manual testing. More generally, these results showcase the promise of multi-agent systems\n",
            "in code generation, past the specific CodePori system. We have provided details about the\n",
            "system in this paper for both transparency and to support the implementation of these tools\n",
            "in practice.\n",
            "Moving forward, our goal is to improve the CodePori system’s capabilities by expanding\n",
            "its understanding to include more programming languages and development frameworks. Ad-\n",
            "ditionally, we plan to enhance the system’s learning abilities by incorporating user feedback,\n",
            "enabling it to produce outputs that meet the specific requirements and preferences of each user\n",
            "and their projects. This will involve the development of more advanced algorithms that can\n",
            "adjust and grow in response to various and changing software development environments.\n",
            "8. Acknowledgment\n",
            "This project is co-funded by the European Union and Business Finland under project\n",
            "BF/Amalia-2023/SW.\n",
            "References\n",
            "[1] C. Treude, Navigating complexity in software engineering: A prototype for comparing\n",
            "gpt-n solutions, in: 2023 IEEE/ACM 5th International Workshop on Bots in Software\n",
            "Engineering (BotSE), IEEE, 2023, pp. 1–5.\n",
            "[2] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al., Improving language under-\n",
            "standing by generative pre-training.\n",
            "[3] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al., Language models\n",
            "are unsupervised multitask learners, OpenAI blog 1 (8) (2019) 9.\n",
            "[4] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agar-\n",
            "wal, K. Slama, A. Ray, et al., Training language models to follow instructions with human\n",
            "feedback, Advances in Neural Information Processing Systems 35 (2022) 27730–27744.\n",
            "[5] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\n",
            "P. Shyam, G. Sastry, A. Askell, et al., Language models are few-shot learners, Advances\n",
            "in neural information processing systems 33 (2020) 1877–1901.\n",
            "[6] L. Belzner, T. Gabor, M. Wirsing, Large language model assisted software engineering:\n",
            "prospects, challenges, and a case study, in: International Conference on Bridging the Gap\n",
            "between AI and Reality, Springer, 2023, pp. 355–374.\n",
            "[7] M. Allamanis, M. Brockschmidt, M. Khademi, Learning to represent programs with\n",
            "graphs, arXiv preprint arXiv:1711.00740.\n",
            "[8] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma,\n",
            "D. Zhou, D. Metzler, et al., Emergent abilities of large language models, arXiv preprint\n",
            "arXiv:2206.07682.\n",
            "19\n",
            "[9] F. Urrutia, R. Araya, Who’s the best detective? large language models vs. traditional ma-\n",
            "chine learning in detecting incoherent fourth grade math answers, Journal of Educational\n",
            "Computing Research 61 (8) (2024) 187–218.\n",
            "[10] X. Hu, H. K. Dam, Future of software engineering@ icse 2023, in: 2023 IEEE/ACM\n",
            "International Conference on Software Engineering: Future of Software Engineering (ICSE-\n",
            "FoSE), IEEE, 2023, pp. 1–3.\n",
            "[11] X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo, D. Lo, J. Grundy, H. Wang,\n",
            "Large language models for software engineering: A systematic literature review, arXiv\n",
            "preprint arXiv:2308.10620.\n",
            "[12] Y. Chae, T. Davidson, Large language models for text classification: From zero-shot\n",
            "learning to fine-tuning, Open Science Foundation.\n",
            "[13] J. W. Rae, S. Borgeaud, T. Cai, K. Millican, J. Hoffmann, F. Song, J. Aslanides, S. Hen-\n",
            "derson, R. Ring, S. Young, et al., Scaling language models: Methods, analysis & insights\n",
            "from training gopher, arXiv preprint arXiv:2112.11446.\n",
            "[14] Z. Rasheed, M. Waseem, K.-K. Kemell, W. Xiaofeng, A. N. Duc, K. Syst¨a, P. Abra-\n",
            "hamsson, Autonomous agents in software development: A vision paper, arXiv preprint\n",
            "arXiv:2311.18440.\n",
            "[15] X. Gu, H. Zhang, S. Kim, Deep code search, in: Proceedings of the 40th International\n",
            "Conference on Software Engineering, 2018, pp. 933–944.\n",
            "[16] F. Lin, D. J. Kim, et al., When llm-based code generation meets the software development\n",
            "process, arXiv preprint arXiv:2403.15852.\n",
            "[17] Q. Gu, Llm-based code generation method for golang compiler testing, in: Proceedings of\n",
            "the 31st ACM Joint European Software Engineering Conference and Symposium on the\n",
            "Foundations of Software Engineering, 2023, pp. 2201–2203.\n",
            "[18] Y. Ishibashi, Y. Nishimura, Self-organized agents: A llm multi-agent framework toward\n",
            "ultra large-scale code generation and optimization, arXiv preprint arXiv:2404.02183.\n",
            "[19] S. Hong, X. Zheng, J. Chen, Y. Cheng, J. Wang, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin,\n",
            "L. Zhou, et al., Metagpt: Meta programming for multi-agent collaborative framework,\n",
            "arXiv preprint arXiv:2308.00352.\n",
            "[20] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,\n",
            "N. Joseph, G. Brockman, et al., Evaluating large language models trained on code, arXiv\n",
            "preprint arXiv:2107.03374.\n",
            "[21] C. Qian, X. Cong, C. Yang, W. Chen, Y. Su, J. Xu, Z. Liu, M. Sun, Communicative\n",
            "agents for software development, arXiv preprint arXiv:2307.07924.\n",
            "[22] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling,\n",
            "F. Gimeno, A. Dal Lago, et al., Competition-level code generation with alphacode, Science\n",
            "378 (6624) (2022) 1092–1097.\n",
            "[23] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih,\n",
            "L. Zettlemoyer, M. Lewis, Incoder: A generative model for code infilling and synthesis,\n",
            "arXiv preprint arXiv:2204.05999.\n",
            "[24] Q. Zheng, X. Xia, X. Zou, Y. Dong, S. Wang, Y. Xue, Z. Wang, L. Shen, A. Wang, Y. Li,\n",
            "et al., Codegeex: A pre-trained model for code generation with multilingual evaluations\n",
            "on humaneval-x, arXiv preprint arXiv:2303.17568.\n",
            "20\n",
            "[25] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\n",
            "Chung, C. Sutton, S. Gehrmann, et al., Palm: Scaling language modeling with pathways,\n",
            "Journal of Machine Learning Research 24 (240) (2023) 1–113.\n",
            "[26] Z. Rasheed, Dataset of the Paper “CodePori:\n",
            "Large Scale System for Autonomous\n",
            "Software Development by Using Multi-Agents”, https://doi.org/10.5281/zenodo.13755415\n",
            "(2024).\n",
            "[27] Z. Rasheed, M. A. Sami, P. Abrahamsson, Codepori, accessed: 2024-09-12 (2024).\n",
            "URL https://github.com/GPT-Laboratory/CodePori\n",
            "[28] R. Gozalo-Brizuela, E. C. Garrido-Merchan, Chatgpt is not all you need. a state of the\n",
            "art review of large generative ai models, arXiv preprint arXiv:2301.04655.\n",
            "[29] D. Rothman, A. Gulli, Transformers for Natural Language Processing: Build, train, and\n",
            "fine-tune deep neural network architectures for NLP with Python, PyTorch, TensorFlow,\n",
            "BERT, and GPT-3, Packt Publishing Ltd, 2022.\n",
            "[30] Y. Li, H. Wen, W. Wang, X. Li, Y. Yuan, G. Liu, J. Liu, W. Xu, X. Wang, Y. Sun, et al.,\n",
            "Personal llm agents: Insights and survey about the capability, efficiency and security,\n",
            "arXiv preprint arXiv:2401.05459.\n",
            "[31] S. Dou, H. Jia, S. Wu, H. Zheng, W. Zhou, M. Wu, M. Chai, J. Fan, C. Huang, Y. Tao,\n",
            "et al., What’s wrong with your code generated by large language models? an extensive\n",
            "study, arXiv preprint arXiv:2407.06153.\n",
            "[32] A. Yadav, M. Singh, Boldly going where no benchmark has gone before: Exposing bias\n",
            "and shortcomings in code generation evaluation, arXiv preprint arXiv:2401.03855.\n",
            "[33] J. Dai, J. Lu, Y. Feng, R. Ruan, M. Cheng, H. Tan, Z. Guo, Mhpp: Exploring the capa-\n",
            "bilities and limitations of language models beyond basic code generation, arXiv preprint\n",
            "arXiv:2405.11430.\n",
            "[34] D. Baidoo-Anu, L. Owusu Ansah, Education in the era of generative artificial intelligence\n",
            "(ai): Understanding the potential benefits of chatgpt in promoting teaching and learning,\n",
            "Available at SSRN 4337484.\n",
            "[35] Z. Rasheed, M. Waseem, A. Ahmad, K.-K. Kemell, W. Xiaofeng, A. N. Duc, P. Abrahams-\n",
            "son, Can large language models serve as data analysts? a multi-agent assisted approach\n",
            "for qualitative data analysis, arXiv preprint arXiv:2402.01386.\n",
            "[36] Y. Cao, S. Li, Y. Liu, Z. Yan, Y. Dai, P. S. Yu, L. Sun, A comprehensive survey of ai-\n",
            "generated content (aigc): A history of generative ai from gan to chatgpt, arXiv preprint\n",
            "arXiv:2303.04226.\n",
            "[37] P. Hacker, A. Engel, M. Mauer, Regulating chatgpt and other large generative ai models,\n",
            "in: Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Trans-\n",
            "parency, 2023, pp. 1112–1123.\n",
            "[38] M. A. Sami, M. Waseem, Z. Rasheed, M. Saari, K. Syst¨a, P. Abrahamsson, Experiment-\n",
            "ing with multi-agent software development: Towards a unified platform, arXiv preprint\n",
            "arXiv:2406.05381.\n",
            "[39] ¨O. Aydın, E. Karaarslan, Is chatgpt leading generative ai? what is beyond expectations?,\n",
            "What is beyond expectations.\n",
            "[40] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser,\n",
            "I. Polosukhin, Attention is all you need, Advances in neural information processing systems\n",
            "30.\n",
            "21\n",
            "[41] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang,\n",
            "Z. Dong, et al., A survey of large language models, arXiv preprint arXiv:2303.18223.\n",
            "[42] J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, J. Han, Large language models can\n",
            "self-improve, arXiv preprint arXiv:2210.11610.\n",
            "[43] A. M. Sami, Z. Rasheed, K.-K. Kemell, M. Waseem, T. Kilamo, M. Saari, A. N. Duc,\n",
            "K. Syst¨a, P. Abrahamsson, System for systematic literature review using multiple ai\n",
            "agents: Concept and an empirical evaluation, arXiv preprint arXiv:2403.08399.\n",
            "[44] Z. Feng, D. Guo, D. Tang, N. Duan, X. Feng, M. Gong, L. Shou, B. Qin, T. Liu, D. Jiang,\n",
            "et al., Codebert: A pre-trained model for programming and natural languages, arXiv\n",
            "preprint arXiv:2002.08155.\n",
            "[45] D. Guo, S. Lu, N. Duan, Y. Wang, M. Zhou, J. Yin, Unixcoder: Unified cross-modal\n",
            "pre-training for code representation, arXiv preprint arXiv:2203.03850.\n",
            "[46] T. Eloundou, S. Manning, P. Mishkin, D. Rock, Gpts are gpts: An early look at the labor\n",
            "market impact potential of large language models, arXiv preprint arXiv:2303.10130.\n",
            "[47] Y. Feng, S. Vanam, M. Cherukupally, W. Zheng, M. Qiu, H. Chen, Investigating code\n",
            "generation performance of chat-gpt with crowdsourcing social data, in: Proceedings of\n",
            "the 47th IEEE Computer Software and Applications Conference, 2023, pp. 1–10.\n",
            "[48] L. Floridi, M. Chiriatti, Gpt-3: Its nature, scope, limits, and consequences, Minds and\n",
            "Machines 30 (2020) 681–694.\n",
            "[49] J. Thiergart, S. Huber, T. ¨Ubellacker, Understanding emails and drafting responses–an\n",
            "approach using gpt-3, arXiv preprint arXiv:2102.03062.\n",
            "[50] A. H¨ornemalm, Chatgpt as a software development tool: The future of development\n",
            "(2023).\n",
            "[51] M. Tufano, D. Drain, A. Svyatkovskiy, S. K. Deng, N. Sundaresan, Unit test case gener-\n",
            "ation with transformers and focal context, arXiv preprint arXiv:2009.05617.\n",
            "[52] W. Ma, S. Liu, W. Wang, Q. Hu, Y. Liu, C. Zhang, L. Nie, Y. Liu, The scope of chatgpt\n",
            "in software engineering: A thorough investigation, arXiv preprint arXiv:2305.12138.\n",
            "[53] N. Nascimento, P. Alencar, D. Cowan, Comparing software developers with chatgpt: An\n",
            "empirical investigation, arXiv preprint arXiv:2305.11837.\n",
            "[54] Z. Rasheed, M. Waseem, K. Syst¨a, P. Abrahamsson, Large language model evaluation via\n",
            "multi ai agents: Preliminary results, arXiv preprint arXiv:2404.01023.\n",
            "[55] F. Quin, D. Weyns, M. Galster, C. C. Silva, A/b testing: a systematic literature review,\n",
            "Journal of Systems and Software (2024) 112011.\n",
            "[56] Z. Zheng, K. Ning, J. Chen, Y. Wang, W. Chen, L. Guo, W. Wang, Towards an\n",
            "understanding of large language models in software engineering tasks, arXiv preprint\n",
            "arXiv:2308.11396.\n",
            "[57] Z. Zheng, K. Ning, Y. Wang, J. Zhang, D. Zheng, M. Ye, J. Chen, A survey of large\n",
            "language models for code: Evolution, benchmarking, and future trends, arXiv preprint\n",
            "arXiv:2311.10372.\n",
            "[58] J. Shin, C. Tang, T. Mohati, M. Nayebi, S. Wang, H. Hemmati, Prompt engineering or\n",
            "fine tuning: An empirical assessment of large language models in automated software\n",
            "engineering tasks, arXiv preprint arXiv:2310.10508.\n",
            "22\n",
            "[59] Y. Wang,\n",
            "W. Wang,\n",
            "S. Joty,\n",
            "S. C. Hoi,\n",
            "Codet5:\n",
            "Identifier-aware unified pre-\n",
            "trained encoder-decoder models for code understanding and generation, arXiv preprint\n",
            "arXiv:2109.00859.\n",
            "[60] S. Black, L. Gao, P. Wang, C. Leahy, S. Biderman, Gpt-neo: Large scale autoregressive\n",
            "language modeling with mesh-tensorflow, If you use this software, please cite it using these\n",
            "metadata 58.\n",
            "[61] B. Wang, A. Komatsuzaki, Gpt-j-6b: A 6 billion parameter autoregressive language model\n",
            "(2021).\n",
            "[62] L. Tunstall, L. Von Werra, T. Wolf, Natural language processing with transformers, ”\n",
            "O’Reilly Media, Inc.”, 2022.\n",
            "[63] F. F. Xu, U. Alon, G. Neubig, V. J. Hellendoorn, A systematic evaluation of large language\n",
            "models of code, in: Proceedings of the 6th ACM SIGPLAN International Symposium on\n",
            "Machine Programming, 2022, pp. 1–10.\n",
            "[64] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, C. Xiong,\n",
            "Codegen: An open large language model for code with multi-turn program synthesis,\n",
            "arXiv preprint arXiv:2203.13474.\n",
            "[65] B. Chen, F. Zhang, A. Nguyen, D. Zan, Z. Lin, J.-G. Lou, W. Chen, Codet: Code\n",
            "generation with generated tests, arXiv preprint arXiv:2207.10397.\n",
            "[66] S. Black, S. Biderman, E. Hallahan, Q. Anthony, L. Gao, L. Golding, H. He, C. Leahy,\n",
            "K. McDonell, J. Phang, et al., Gpt-neox-20b: An open-source autoregressive language\n",
            "model, arXiv preprint arXiv:2204.06745.\n",
            "[67] S. Arora, A. Narayan, M. F. Chen, L. Orr, N. Guha, K. Bhatia, I. Chami, C. Re, Ask me\n",
            "anything: A simple strategy for prompting language models, in: The Eleventh Interna-\n",
            "tional Conference on Learning Representations, 2022.\n",
            "[68] C. Wang, Q. Dong, X. Wang, H. Wang, Z. Sui, Statistical dataset evaluation: Reliability,\n",
            "difficulty, and validity, arXiv preprint arXiv:2212.09272.\n",
            "[69] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang,\n",
            "et al., Qwen technical report, arXiv preprint arXiv:2309.16609.\n",
            "[70] O. Contributors, Opencompass: A universal evaluation platform for foundation models,\n",
            "GitHub repository.\n",
            "[71] S. Golchin, M. Surdeanu, Time travel in llms: Tracing data contamination in large lan-\n",
            "guage models, arXiv preprint arXiv:2308.08493.\n",
            "[72] M. Riddell, A. Ni, A. Cohan, Quantifying contamination in evaluating code generation\n",
            "capabilities of language models, arXiv preprint arXiv:2403.04811.\n",
            "[73] M. Roberts, H. Thakur, C. Herlihy, C. White, S. Dooley, To the cutoff... and beyond? a\n",
            "longitudinal perspective on llm data contamination, in: The Twelfth International Con-\n",
            "ference on Learning Representations, 2023.\n",
            "[74] P. Runeson, M. H¨ost, Guidelines for conducting and reporting case study research in\n",
            "software engineering, Empirical software engineering 14 (2009) 131–164.\n",
            "[75] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,\n",
            "J. Altenschmidt, S. Altman, S. Anadkat, et al., Gpt-4 technical report, arXiv preprint\n",
            "arXiv:2303.08774.\n",
            "[76] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. Li,\n",
            "et al., Deepseek-coder: When the large language model meets programming–the rise of\n",
            "code intelligence, arXiv preprint arXiv:2401.14196.\n",
            "23\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Summarizer agent\n",
        "\n",
        "\n",
        "summarizer_agent = AssistantAgent(\n",
        "    name=\"summarizer\",\n",
        "    model_client=text_model_client,\n",
        "    system_message=\"\"\"\n",
        "        You are a qualified and experienced reviewer and critic of scientific papers.\n",
        "        You will be given a paper text and you will summarize it as your response.\n",
        "        You must fit the summarization in 300 words.\n",
        "        It is very important to depict the key contributions of the paper.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# CHECK IF SUMMARIZING WORKS\n",
        "\n",
        "async def summarizer_run(paper_text: str) -> None:\n",
        "    response = await summarizer_agent.on_messages(\n",
        "        [TextMessage(content=paper_text, source=\"user\")],\n",
        "        cancellation_token=CancellationToken(),\n",
        "    )\n",
        "\n",
        "    #print(paper_text)\n",
        "    #print(response.inner_messages)\n",
        "    print(response.chat_message.models_usage)\n",
        "    print(response.chat_message.content)\n",
        "\n",
        "\n",
        "\n",
        "# Use asyncio.run(assistant_run()) when running in a script.\n",
        "await summarizer_run(test_paper_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts9tUs-6XyhP",
        "outputId": "77b22ae4-18e0-42b3-9932-2c40e048a72a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RequestUsage(prompt_tokens=23933, completion_tokens=243)\n",
            "The paper \"CodePori: Large-Scale System for Autonomous Software Development Using Multi-Agent Technology\" presents a novel system designed to automate code generation for large and complex software projects using large language models (LLMs). The system utilizes six LLM-based multi-agents, each responsible for a specific task in the software development process, such as system design, code development, code review, code verification, and test engineering. The paper discusses the potential benefits of such a system in enhancing productivity and reducing time-to-market for complex software solutions.\n",
            "\n",
            "The proposed system, CodePori, was evaluated using the HumanEval benchmark and manually tested. The results show that CodePori improved code accuracy and efficiency by 89% and 85%, respectively, compared to existing systems. The paper also discusses the challenges faced by current systems in handling complex tasks and the potential benefits of utilizing LLM-based multi-agent systems in software development. The paper contributes to both industry and academia by demonstrating the transformative potential of LLM-based agents in software engineering and opening new opportunities for broader adoption in both industry and academia.\n"
          ]
        }
      ]
    }
  ]
}
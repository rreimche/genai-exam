{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4qMDh/3c6zePZn0wC6ikz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rreimche/genai-exam/blob/main/LitRev2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prerequisites:\n",
        "\n",
        "- Google Colab\n",
        "- Access to a Google Colab Secret named \"HF_API_TOKEN\" containing a Huggingface API token with read access."
      ],
      "metadata": {
        "id": "FxKuuRqaa1ti"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "XSEOGHM1apvo"
      },
      "outputs": [],
      "source": [
        "#@title Settings\n",
        "from google.colab import userdata\n",
        "\n",
        "download_dir = \"papers\"  # directory name to download papers to revie to\n",
        "huggingface_apikey = userdata.get('HF_API_TOKEN')  # use colab secrets to store the huggingface api key\n",
        "groq_key = userdata.get('GROQ_KEY')  #  use colab secrets to store the huggingface api key\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Declare bibtex references and parse them into a variable for later usage\n",
        "\n",
        "\n",
        "bibtext_references = \"\"\"\n",
        "\n",
        "\n",
        "@misc{qian2024chatdevcommunicativeagentssoftware,\n",
        "      title={ChatDev: Communicative Agents for Software Development},\n",
        "      author={Chen Qian and Wei Liu and Hongzhang Liu and Nuo Chen and Yufan Dang and Jiahao Li and Cheng Yang and Weize Chen and Yusheng Su and Xin Cong and Juyuan Xu and Dahai Li and Zhiyuan Liu and Maosong Sun},\n",
        "      year={2024},\n",
        "      eprint={2307.07924},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.SE},\n",
        "      url={https://arxiv.org/abs/2307.07924},\n",
        "},\n",
        "\n",
        "@misc{nguyen2024agilecoderdynamiccollaborativeagents,\n",
        "      title={AgileCoder: Dynamic Collaborative Agents for Software Development based on Agile Methodology},\n",
        "      author={Minh Huynh Nguyen and Thang Phan Chau and Phong X. Nguyen and Nghi D. Q. Bui},\n",
        "      year={2024},\n",
        "      eprint={2406.11912},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.SE},\n",
        "      url={https://arxiv.org/abs/2406.11912},\n",
        "},\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "hghBKNhCa_3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "!pip install -q autogen arxiv scholarly crossrefapi beautifulsoup4 requests cloudscraper pymupdf nltk autogen-agentchat autogen-ext[openai] groq pymupdf;\n",
        "!pip install -q --pre bibtexparser;"
      ],
      "metadata": {
        "id": "Asy4oaZmbF2F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a13bf7-bd8d-4397-eb11-9a27d2c44fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/55.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m642.5/642.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.7/99.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.2/122.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.7/233.7 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.8/125.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m53.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.9/492.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for bibtexparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for free-proxy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opentelemetry-sdk 1.16.0 requires opentelemetry-api==1.16.0, but you have opentelemetry-api 1.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preparation for agents: model client connections\n",
        "from autogen_agentchat.agents import AssistantAgent\n",
        "from autogen_agentchat.messages import TextMessage\n",
        "from autogen_core.tools import FunctionTool\n",
        "from autogen_core.models import UserMessage\n",
        "#from autogen_agentchat.ui import Console\n",
        "from autogen_core import CancellationToken\n",
        "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
        "\n",
        "# INSTANTIATE MODEL CLIENTS\n",
        "\n",
        "# This client will be used for paper summarization\n",
        "text_model_client = OpenAIChatCompletionClient(\n",
        "    #model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\",\n",
        "    #model=\"facebook/bart-large-xsum\",\n",
        "    model=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    #model=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
        "    base_url=\"https://router.huggingface.co/hf-inference/v1\",\n",
        "    api_key=huggingface_apikey,\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "\n",
        "# We need another inference point for downloader agent,\n",
        "# because huggingface can't serve reflection on tool use\n",
        "# so that autogen agent understands it\n",
        "tool_model_client = OpenAIChatCompletionClient(\n",
        "    #model=\"llama3-70b-8192\",\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    #model=\"deepseek-r1-distill-qwen-32b\",\n",
        "    base_url=\"https://api.groq.com/openai/v1\",\n",
        "    api_key=groq_key,\n",
        "    model_info={\n",
        "        \"vision\": False,\n",
        "        \"function_calling\": True,\n",
        "        \"json_output\": False,\n",
        "        \"family\": \"unknown\",\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# CHECK CONNECTION\n",
        "\n",
        "async def check_model_connect() -> None:\n",
        "    messages = [\n",
        "        UserMessage(content=\"What is the capital of France?\", source=\"user\"),\n",
        "    ]\n",
        "    response = await text_model_client.create(messages=messages)\n",
        "    response_downloader = await tool_model_client.create(messages=messages)\n",
        "\n",
        "    print(response.content)\n",
        "    print(response_downloader.content)\n",
        "\n",
        "await check_model_connect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oheTZ2eGbS48",
        "outputId": "6d6b53bb-e088-43a0-bdb9-8bf773585e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is Paris. It is also one of the most famous cities in the world, known for its architecture, art, fashion, and cuisine. Paris is home to many iconic landmarks, such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral.\n",
            "The capital of France is Paris.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Downloader agent\n",
        "import arxiv\n",
        "import os\n",
        "import datetime\n",
        "import json\n",
        "import sys\n",
        "from dataclasses import dataclass\n",
        "from autogen_core.models import ChatCompletionClient\n",
        "from autogen_core import message_handler\n",
        "\n",
        "# HELPER AND TOOL FUNCTIONS\n",
        "\n",
        "def maybe_create_dir(donwload_dir: str) -> None:\n",
        "    \"\"\"\n",
        "        A helper function to create the downloads directory\n",
        "        in colab root if the directory is not yet present.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        os.makedirs(donwload_dir, exist_ok=True)\n",
        "        #print(f\"Directory '{download_dir}' created successfully.\")\n",
        "    except OSError as e:\n",
        "        raise Exception(f\"Error creating directory '{download_dir}': {e}\")\n",
        "\n",
        "def download_paper(eprint_id: str) -> str:\n",
        "    \"\"\"\n",
        "        Receives a eprint_id of a paper from arxiv, downloads the specified paper\n",
        "        and saves it in downloads directory.\n",
        "        The filepath of the downloaded paper is in the resulting dictionary\n",
        "        with the key \"filepath\".\n",
        "\n",
        "        :param eprint_id: string, the arxiv eprint_id.\n",
        "        :return: string with filepath of the downloaded paper.\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(eprint_id) > 0, \"eprint_id cannot be empty\"\n",
        "\n",
        "    result = {\n",
        "        \"eprint_id\": eprint_id,\n",
        "        \"success\": False,\n",
        "        \"filepath\": None,\n",
        "        }\n",
        "\n",
        "\n",
        "    if eprint_id:\n",
        "        #print(f\"{datetime.datetime.now()} - Trying arXiv for: {eprint_id}\")\n",
        "        try:\n",
        "            client = arxiv.Client()\n",
        "            search = arxiv.Search(id_list=[eprint_id], max_results=1)\n",
        "\n",
        "            #arxiv_result = next(search.results(), None)\n",
        "            arxiv_result = next(client.results(search))\n",
        "            if arxiv_result:\n",
        "                #print(f\"  Found on arXiv: {arxiv_result.title}\")\n",
        "                maybe_create_dir(download_dir)\n",
        "                filepath = os.path.join(download_dir, f\"{arxiv_result.title.replace(' ', '_')}.pdf\")\n",
        "                arxiv_result.download_pdf(filename=filepath)\n",
        "                result[\"success\"] = True\n",
        "                result[\"filepath\"] = filepath\n",
        "                #print(f\"  Download successful: {filepath}\")\n",
        "            else:\n",
        "                # print(f\"  No arXiv result found for ID {eprint_id}.\")\n",
        "                raise Exception(f\"ERROR: No arXiv result found for ID {eprint_id}.\")\n",
        "                #return f\"ERROR: No arXiv result found for ID {eprint_id}.\"\n",
        "\n",
        "        except Exception as e:\n",
        "            #print(f\"arXiv download failed for: {eprint_id}: {e}\")\n",
        "            raise Exception(f\"ERROR: arXiv download failed for: {eprint_id}: {e}\")\n",
        "            #return f\"ERROR: arXiv download failed for: {eprint_id}: {e}\"\n",
        "\n",
        "    # Check if the download was NOT successful\n",
        "    if not result[\"success\"]:\n",
        "        #print(f\"{datetime.datetime.now()} - Download failed or not attempted for: {eprint_id}\")\n",
        "        raise Exception(f\"ERROR: Download failed or not attempted for: {eprint_id}\")\n",
        "        #return f\"ERROR: Download failed or not attempted for: {eprint_id}\"\n",
        "\n",
        "    return result[\"filepath\"]\n",
        "\n",
        "\n",
        "# DOWNLOADER AGENT WITH TOOL\n",
        "\n",
        "def create_downloader(given_name: str) -> AssistantAgent:\n",
        "    downloader_tool = FunctionTool(download_paper, description=\"Given a string denoting an eprint_id of a page on arxiv, downloads a paper\")\n",
        "\n",
        "    return AssistantAgent(\n",
        "        name=given_name,\n",
        "        description=\"An agent that uses downloader tool to download a paper from Arxiv identified by a eprint_id\",\n",
        "        model_client=tool_model_client,\n",
        "        tools=[downloader_tool],\n",
        "        reflect_on_tool_use=True,\n",
        "        system_message=\"\"\"\n",
        "            Use downloader tool to download specified paper(s) from arxiv.\n",
        "            Here is an example of how you must respond in success case:\n",
        "              'mypapers/veryinterstingpaper.pdf'\n",
        "\n",
        "            Very important: You MUST return the full filepath of the downloaded\n",
        "            paper EXACTLY as it was given by the downloader tool.\n",
        "\n",
        "            If download was not successful, respond like this:\n",
        "              'ERROR: Download for paper with eprint id XXX failed for the following reason: REASON'.\n",
        "          \"\"\",\n",
        "    )\n",
        "\n",
        "# CHECK IF DOWNLOAD WORKS\n",
        "\n",
        "async def downloader_run() -> None:\n",
        "    response = await create_downloader(\"Test\").on_messages(\n",
        "        [TextMessage(content=\"Download the paper with eprint_id 2402.01411\", source=\"user\")],\n",
        "        cancellation_token=CancellationToken(),\n",
        "    )\n",
        "\n",
        "\n",
        "    #print(response.inner_messages)\n",
        "    #print(response.chat_message)\n",
        "    print(response.chat_message.content)\n",
        "\n",
        "\n",
        "\n",
        "# Use asyncio.run(assistant_run()) when running in a script.\n",
        "#await downloader_run()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "oirHwIBNc_-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Agents to get paper text: Informant and Parser\n",
        "import arxiv\n",
        "from typing import Dict\n",
        "import pymupdf\n",
        "import requests\n",
        "from autogen_agentchat.ui import Console\n",
        "\n",
        "def get_arxiv_data(eprint_id: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Given arxiv eprint_id of a scientific paper, returns title of the paper\n",
        "    along with the PDF URL.\n",
        "\n",
        "    :param eprint_id: string, the arxiv eprint_id.\n",
        "    :return: dictionary with keys \"title\" and \"pdf_url\".\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(eprint_id) > 0, \"eprint_id cannot be empty\"\n",
        "\n",
        "    client = arxiv.Client()\n",
        "    search = arxiv.Search(id_list=[eprint_id], max_results=1)\n",
        "\n",
        "    try:\n",
        "\n",
        "        arxiv_result = next(client.results(search))\n",
        "        if arxiv_result:\n",
        "            return {\n",
        "                \"title\": arxiv_result.title,\n",
        "                \"pdf_url\": arxiv_result.pdf_url,\n",
        "            }\n",
        "        else:\n",
        "            raise Exception(f\"ERROR: No arXiv result found for ID {eprint_id}.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"ERROR: getting data from arxiv failed for: {eprint_id}: {e}\")\n",
        "\n",
        "\n",
        "def parse_paper(url: str) -> str:\n",
        "    \"\"\"\n",
        "    Given URL of a PDF as a string, returns the contents as a string\n",
        "\n",
        "    :param url: string, the URL of the PDF.\n",
        "    :return: string with the contents of the PDF.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "\n",
        "        r = requests.get(url)\n",
        "        data = r.content\n",
        "        doc = pymupdf.Document(stream=data)\n",
        "        text = chr(12).join([page.get_text() for page in doc])\n",
        "        return text\n",
        "\n",
        "    except Exception as e:\n",
        "        # print(f\"Error parsing {title} ({filepath}): {e}\")\n",
        "        raise Exception(f\"ERROR: Error parsing the paper at {url}: {e}\")\n",
        "\n",
        "\n",
        "async def get_concept(eprint_id: str) -> str:\n",
        "    \"\"\"\n",
        "    Takes a eprint id from arxiv and returns the summarization of the related paper.\n",
        "\n",
        "    :param eprint_id: string, the arxiv eprint_id.\n",
        "    :return: string with PDF contents.\n",
        "    \"\"\"\n",
        "\n",
        "    assert len(eprint_id) > 0, \"eprint_id cannot be empty\"\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        arxiv_data = get_arxiv_data(eprint_id)\n",
        "        #print(arxiv_data[\"pdf_url\"])\n",
        "        text = parse_paper(arxiv_data[\"pdf_url\"])\n",
        "\n",
        "        concept_prompt = f\"\"\"\n",
        "          You are a professor of computer science specialized in multi agent systems\n",
        "          and generative AI. Your task ist to summarize a certain scientific paper.\n",
        "\n",
        "          Here is the text of the paper:\n",
        "\n",
        "          -------start of paper--------\n",
        "          {text}\n",
        "          -------end of paper--------\n",
        "\n",
        "          Use the provided text to write the summarization.\n",
        "          Write consice and clear using the highest academic standards.\n",
        "          It is very important to provide short descriptions of key contributions.\n",
        "          Fit your answer in 500 words.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "        concept = await text_model_client.create([UserMessage(content=concept_prompt, source=\"user\")])\n",
        "        #print(concept)\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"ERROR: Error downloading, parsing or summarizing the paper identified by eprint id {eprint_id}: {e}\")\n",
        "\n",
        "    return concept\n",
        "\n",
        "\n",
        "tool_getsummary = FunctionTool(\n",
        "    get_concept,\n",
        "    description = \"Takes a eprint id of a paper published on arxiv and returns a summarization of the related paper.\"\n",
        ")\n",
        "\n",
        "\n",
        "paper_summarizer = AssistantAgent(\n",
        "    name=\"parser\",\n",
        "    description=\"Provided with arxiv eprint id, returns a summarization of the related paper\",\n",
        "    model_client=tool_model_client,\n",
        "    tools=[tool_getsummary],\n",
        "    reflect_on_tool_use=True,\n",
        "    system_message=\"\"\"\n",
        "        You are a a professor of computer science. Use eprint ids of papers\n",
        "        published at Arxiv to use the tool you're equipped with (tool_getsummary),\n",
        "        which will give you the summarization of the related paper.\n",
        "        Give the text of the paper back as your answer. Answer concise\n",
        "        and structured.\n",
        "\n",
        "        You are not allowed to do any other work like planning, writing summarizations or reviews.\n",
        "    \"\"\"\n",
        "\n",
        ")\n",
        "\n",
        "# CHECK IF WORKS\n",
        "\n",
        "async def get_paper_summary_run() -> None:\n",
        "    #response_i = await paper_informant.on_messages(\n",
        "    #    [TextMessage(content=\"Give me the infos of the paper with eprint_id 2307.07924\", source=\"user\")],\n",
        "    #    cancellation_token=CancellationToken(),\n",
        "    #)\n",
        "\n",
        "    #prev_answer = response_i.chat_message.content\n",
        "\n",
        "    #print(prev_answer)\n",
        "\n",
        "    #response_p = await paper_parser.on_messages(\n",
        "    #    [TextMessage(content=f\"Give me the text of paper, which url is specified in the following text: {prev_answer}\", source=\"user\")],\n",
        "    #    cancellation_token=CancellationToken(),\n",
        "    #)\n",
        "\n",
        "    # for deeper debugging\n",
        "    #await Console(\n",
        "    #    paper_conceptualizer.on_messages_stream(\n",
        "    #        [TextMessage(content=f\"Give me the concept of a paper with eprint_id 2307.07924\", source=\"user\")],\n",
        "    #        cancellation_token=CancellationToken(),\n",
        "    #    ),\n",
        "    #    output_stats=True,  # Enable stats printing.\n",
        "    #)\n",
        "\n",
        "    response = await paper_summarizer.on_messages(\n",
        "        [TextMessage(content=\"Give me a summarization of the paper with eprint_id 2307.07924\", source=\"user\")],\n",
        "        cancellation_token=CancellationToken(),\n",
        "    )\n",
        "\n",
        "    #print(response.inner_messages)\n",
        "    #print(response.chat_message)\n",
        "    #print(response.chat_message.content)\n",
        "\n",
        "    return response.chat_message.content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Use asyncio.run(assistant_run()) when running in a script.\n",
        "test_paper_summary = await get_paper_summary_run()\n",
        "print(test_paper_summary)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vxgO7DIjdVIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f472cb-3012-4476-ca28-97df4180df34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper with eprint_id 2307.07924 introduces ChatDev, a software development framework that utilizes large language models and multiprovised agents to streamline the development process. ChatDev offers various auxiliary roles, including a requirements analyst, a professional programmer, and a tester, who perform their functions in a designated phase. The core of ChatDev lies in the chat chain, which segments each phase into smaller subtasks, enabling multi-turn dialogues between the agents to collaboratively propose and build solutions. The framework also employs a mechanism called communicative dehallucination to minimize \"coding hallucinations,\" wherein the assistant actively seeks more detailed information before providing responses to avoid generating incomplete or inconsistent code. The use of ChatDev improves various performance metrics, including completeness, executability, and consistency, and is superior to related LLM-based software development methods.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Reviewer agent\n",
        "from typing import List\n",
        "\n",
        "\n",
        "reviewer = AssistantAgent(\n",
        "    name=\"reviewer\",\n",
        "    description=\"\"\"\n",
        "        Given a several summarizations (short description of important paper contents)\n",
        "        for several scientific papers, writes a scientific literature review.\n",
        "    \"\"\",\n",
        "    model_client=text_model_client,\n",
        "    #system_message=\"\"\"\n",
        "#        You are a professor of computer science preparing a review\n",
        "#        of recent publications on the topic of multi-agent llm-based systems\n",
        "#        for end-to-end software development (such systems receive requirements\n",
        "#        as input and deliver a ready system as output with optional system-user\n",
        "#        interactions inbetween). You will receive the conceptualisations of\n",
        "#        several papers (that is, consice enumeration of important contributions of the papers)\n",
        "#        and write a concise and factually detailed literature review.\n",
        "#        Stick to scientific standards regarding style and rigour.\n",
        "#        You must fit your review in 500 words.\n",
        "\n",
        "#        It is very important to depict the key contributions of the papers.\n",
        "#\"\"\",\n",
        "    system_message=\"\"\"\n",
        "        You are a professor of computer science preparing a review\n",
        "        of recent publications on the topic of multi-agent llm-based systems\n",
        "        for end-to-end software development (such systems receive requirements\n",
        "        as input and deliver a ready system as output with optional system-user\n",
        "        interactions inbetween). Use provided conceptualisations of\n",
        "        several papers (that is, consice enumeration of important contributions of the papers)\n",
        "        and write a concise and factually detailed literature review.\n",
        "        Stick to scientific standards regarding style and rigour, but provide\n",
        "        only the review text, omitting authors, refences and other metainformation.\n",
        "        You must fit your review in 500 words.\n",
        "\n",
        "        It is very important to depict the key contributions of the papers.\n",
        "\n",
        "        You do not do any other work like downloading, parsing, writing conceptualizations or summarizations.\n",
        "\"\"\",\n",
        ")\n",
        "\n",
        "\n",
        "# CHECK IF REVIEWING WORKS\n",
        "\n",
        "async def reviewer_run(summarizations: List[str]) -> str:\n",
        "\n",
        "    prompt_rev = f\"Here are {len(summarizations)} paper summarizations that you need to review:\\n\"\n",
        "    for summarization in summarizations:\n",
        "        prompt_rev += f\"------start summarizations-------\\n{summarization}\\n------end summarizations-------\\n\"\n",
        "    prompt_rev += \"Write the scientific literature review.\"\n",
        "\n",
        "\n",
        "    response = await reviewer.on_messages(\n",
        "       [TextMessage(content=prompt_rev, source=\"user\")],\n",
        "       cancellation_token=CancellationToken(),\n",
        "    )\n",
        "\n",
        "    #print(paper_text)\n",
        "    #print(response.inner_messages)\n",
        "    #print(response.chat_message.models_usage)\n",
        "    return response.chat_message.content\n",
        "\n",
        "\n",
        "\n",
        "# Use asyncio.run(assistant_run()) when running in a script.\n",
        "review = await reviewer_run([test_paper_summary, test_paper_summary])\n",
        "print(review)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5reIaye0yi4x",
        "outputId": "a75f1250-aecf-42c5-ab15-259ffd82b285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In this literature review, we focus on two recent publications that propose the use of multi-agentllm-based systems for end-to-end software development. The first paper, with eprint_id 2307.07924, introduces ChatDev, a novel software development framework. ChatDev leverages large language models and multiprovider agents to streamline the development process, consisting of auxiliary roles such as a requirements analyst, professional programmer, and tester, each performing their functions in a designated phase.\n",
            "\n",
            "The core of ChatDev is its chat chain, which segments each phase into smaller subtasks, enabling multi-turn dialogues between the agents to collaboratively propose and build solutions. To minimize \"coding hallucinations,\" a common issue in such systems, ChatDev employs a mechanism called communicative dehallucination. This mechanism empowers the assistant to actively seek more detailed information before providing responses, thereby reducing the generation of incomplete or inconsistent code.\n",
            "\n",
            "The paper validates ChatDev's performance against relatedllm-based software development methods, demonstrating improvements in several key performance metrics, including completeness, executability, and consistency. The results indicate that ChatDev is superior in these aspects, making it an attractive solution for improving software development efficiency and reducing errors.\n",
            "\n",
            "The second paper, also with eprint_id 2307.07924, presents a strikingly similar framework, albeit with subtle differences. It incorporates a similar architecture, utilizing multiprovider agents and a chat chain, as well as the communicative dehallucination mechanism. However, there seems to be no significant distinction in the reported performance metrics or methodologies employed, making it challenging to discern the unique contributions of this paper in comparison to the first.\n",
            "\n",
            "In conclusion, both papers demonstrate the potential of multi-agent llm-based systems for enhancing end-to-end software development. Particularly, the ChatDev framework stands out due to its impressive performance improvements and strategic use of communicative dehallucination to minimize coding errors. Future research could explore methods to further improve these systems, address their limitations, and compare them with other existing software development approaches for a comprehensive analysis of their effectiveness.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Planner\n",
        "\n",
        "planner = AssistantAgent(\n",
        "    name=\"planner\",\n",
        "    description=\"Plans the literature review process, assigns tasks to other agents.\",\n",
        "    model_client=tool_model_client,\n",
        "    system_message=\"\"\"\n",
        "      Your task is to plan scientific review process for given papers, assigning tasks to agents.\n",
        "      You have several agents in your disposition:\n",
        "      1. Informant: given a eprint_id of a paper published on arxiv,\n",
        "      gets the paper metadata, including title and PDF URL.\n",
        "      2. Parser: given a singe URL of a paper PDF file, gets the file, parses it and returns the text.\n",
        "      3. Conceptualizer: given a single text of a scientific paper, this agent will write\n",
        "      a conceptualization -- structure and key contributions delivered by the paper.\n",
        "      4. Summarizer: given a single conceptualization of a paper (title, structure of\n",
        "      the paper, key points and contributions), writes a summarization like a paper abstract.\n",
        "      5. Reviewer: given several summarizations (short description of important paper contents)\n",
        "      for several scientific papers, writes a scientific literature review on these papers.\n",
        "\n",
        "      When assigning tasks, use this format:\n",
        "      1. <agent> : <task>\n",
        "\n",
        "      Only plan the work of others, do not do any work yourself.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Check planner\n",
        "async def planner_run(task: str) -> None:\n",
        "\n",
        "\n",
        "    await Console(\n",
        "        planner.on_messages_stream(\n",
        "            [TextMessage(content=task, source=\"user\")],\n",
        "            cancellation_token=CancellationToken(),\n",
        "        ),\n",
        "        output_stats=True,\n",
        "    )\n",
        "\n",
        "    #print(paper_text)\n",
        "    #print(response.inner_messages)\n",
        "    #print(response.chat_message.models_usage)\n",
        "\n",
        "await planner_run(\"Write a literature review on the papers, identified by the following arxiv eprint ids: 2307.07924, 2406.11912\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mj0PbKrwCTpp",
        "outputId": "dc1a50ef-d72f-44fa-ae02-7ce278251670"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- planner ----------\n",
            "To generate a literature review on the given papers, the following tasks should be assigned to the agents:\n",
            "\n",
            "1. Informant : Get the paper metadata for eprint_id 2307.07924\n",
            "2. Informant : Get the paper metadata for eprint_id 2406.11912\n",
            "3. Parser : Parse the PDF file from the URL provided by the Informant for eprint_id 2307.07924\n",
            "4. Parser : Parse the PDF file from the URL provided by the Informant for eprint_id 2406.11912\n",
            "5. Conceptualizer : Conceptualize the text parsed by the Parser for eprint_id 2307.07924\n",
            "6. Conceptualizer : Conceptualize the text parsed by the Parser for eprint_id 2406.11912\n",
            "7. Summarizer : Summarize the conceptualization for eprint_id 2307.07924\n",
            "8. Summarizer : Summarize the conceptualization for eprint_id 2406.11912\n",
            "9. Reviewer : Write a scientific literature review using the summarizations for both eprint_id 2307.07924 and 2406.11912\n",
            "\n",
            "After these tasks are completed, the Reviewer will have generated a literature review on the papers identified by the given arxiv eprint ids.\n",
            "[Prompt tokens: 297, Completion tokens: 274]\n",
            "---------- Summary ----------\n",
            "Number of inner messages: 0\n",
            "Total prompt tokens: 297\n",
            "Total completion tokens: 274\n",
            "Duration: 2.41 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preparing the task for the system\n",
        "from bibtexparser.bparser import BibTexParser\n",
        "from bibtexparser.bwriter import BibTexWriter\n",
        "from bibtexparser.bibdatabase import BibDatabase\n",
        "from bibtexparser.customization import convert_to_unicode\n",
        "from typing import List\n",
        "\n",
        "\n",
        "def parse_bibtex_string(bibtex_string):\n",
        "\n",
        "    if not bibtex_string:\n",
        "        return []  # Handle empty input\n",
        "\n",
        "    try:\n",
        "        parser = BibTexParser()\n",
        "        parser.customization = convert_to_unicode\n",
        "        parser.ignore_nonstandard_strings = True  # Avoid errors with non-standard fields\n",
        "        db = parser.parse(bibtex_string)\n",
        "        return db.entries\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing BibTeX string: {e}\")\n",
        "        return None  # Indicate parsing failure\n",
        "\n",
        "def check_parsed_bibtex(items: List[dict])  -> None:\n",
        "    for entry in bibtex_items:\n",
        "        print(f\"Entry Type: {entry['ENTRYTYPE']}\")\n",
        "        print(f\"  Key: {entry['ID']}\")\n",
        "        for key, value in entry.items():\n",
        "            if key not in ['ENTRYTYPE', 'ID']:\n",
        "                print(f\"  {key}: {value}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "\n",
        "bibtex_items = parse_bibtex_string(bibtext_references)\n",
        "\n",
        "#check_parsed_bibtex(bibtex_items)\n",
        "\n",
        "eprints = \", \".join([item['eprint'] for item in bibtex_items])\n",
        "\n",
        "task = f\"Write a literature review on the papers, identified by the following arxiv eprint ids: {eprints}\"\n",
        "\n",
        "print(f\"THE TASK\\n{task}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "CVveN-Ln_2w_",
        "outputId": "6856120a-509d-4f0e-845c-57a2ee30b033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THE TASK\n",
            "Write a literature review on the papers, identified by the following arxiv eprint ids: 2307.07924, 2406.11912\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Putting it all together\n",
        "from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination\n",
        "from autogen_agentchat.teams import SelectorGroupChat\n",
        "from autogen_agentchat.ui import Console\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "text_mention_termination = TextMentionTermination(\"APPROVE\")\n",
        "max_messages_termination = MaxMessageTermination(max_messages=5)  # 4 phases x 6 papers + reviewer + user feedback + a bit of slack\n",
        "termination = text_mention_termination | max_messages_termination\n",
        "\n",
        "selector_prompt = \"\"\"\n",
        "  Select an agent to perform the given task. For this, you have the following\n",
        "  agent roles:\n",
        "\n",
        "  {roles}\n",
        "\n",
        "  It is important to consider conversation context:\n",
        "  {history}\n",
        "\n",
        "  Read the above conversation, then select an agent from {participants} to perform the next task.\n",
        "  Make sure the planner agent has assigned tasks before other agents start working.\n",
        "  Only select one agent.\n",
        "\"\"\"\n",
        "\n",
        "team = SelectorGroupChat(\n",
        "    [planner, paper_informant, paper_parser, conceptualizer, summarizer, reviewer],\n",
        "    model_client=tool_model_client,\n",
        "    termination_condition=termination,\n",
        "    selector_prompt=selector_prompt,\n",
        "    allow_repeated_speaker=True,  # Allow an agent to speak multiple turns in a row.\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "l9iN0KER6tBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Showtime\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "await Console(team.run_stream(task=task))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buyARJW1AEV1",
        "outputId": "0128e1ac-02d6-4f82-997e-5540c70bb294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- user ----------\n",
            "Write a literature review on the papers, identified by the following arxiv eprint ids: 2307.07924, 2406.11912\n",
            "---------- planner ----------\n",
            "To write a literature review, the following tasks should be assigned to the agents:\n",
            "\n",
            "1. **Informant**: Get the paper metadata for eprint_id 2307.07924\n",
            "2. **Informant**: Get the paper metadata for eprint_id 2406.11912\n",
            "3. **Parser**: Parse the PDF file from the URL provided by the Informant for eprint_id 2307.07924\n",
            "4. **Parser**: Parse the PDF file from the URL provided by the Informant for eprint_id 2406.11912\n",
            "5. **Conceptualizer**: Conceptualize the text parsed by the Parser for eprint_id 2307.07924\n",
            "6. **Conceptualizer**: Conceptualize the text parsed by the Parser for eprint_id 2406.11912\n",
            "7. **Summarizer**: Summarize the conceptualization for eprint_id 2307.07924\n",
            "8. **Summarizer**: Summarize the conceptualization for eprint_id 2406.11912\n",
            "9. **Reviewer**: Write a scientific literature review using the summarizations for both eprint_id 2307.07924 and 2406.11912\n",
            "\n",
            "The Reviewer will then use the summarizations to write a literature review. Please note that I'm planning the work of others, and the actual literature review will be written by the Reviewer after completing these tasks.\n",
            "---------- informant ----------\n",
            "[FunctionCall(id='call_yk32', arguments='{\"eprint_id\": \"2307.07924\"}', name='get_arxiv_data'), FunctionCall(id='call_0jw8', arguments='{\"eprint_id\": \"2406.11912\"}', name='get_arxiv_data')]\n"
          ]
        }
      ]
    }
  ]
}
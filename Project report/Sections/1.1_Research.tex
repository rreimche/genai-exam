
\paragraph{START}

\subsection{Research and tool selection}

\subsubsection{Software and Platforms}
\label{choice:software}

Our multi agent system is developed using the AutoGen\footnote{https://microsoft.github.io/autogen/stable/} framework from Microsoft. The framework allows fast and practical development of multi-agent LLM-based systems. This choice implicates the programming language we use, it is python. The version of the language corresponds to the one available by default on the Google Colab\footnote{https://colab.research.google.com} platform, that we use as runtime environment. The python version is 3 and the minor version is not shown by the runtime information panel or configuration options. Besides that we use the following python libraries:

\begin{itemize}
	\item arxiv (to get paper information from Arxiv\footnote{https://arxiv.org/})
	\item requests (to get PDF files from Arxiv)
	\item pymupdf (to parse PDF files)
	\item bibtexparser (to parse bibtex references)
\end{itemize}

We also decided to use two model inference providers: Groq\footnote{https://groq.com} and HuggingFace\footnote{http://huggingface.co}. These choices, together with the choice of runtime environment are motivated by practicality: in our opinion it is more convenient to use free cloud-based infrastructure than to configure and maintain a local one.

AutoGen is well documented and practical framework, required by the exam task statement and Google Colab is a reliable platform whith free tier is sufficient for the task. Besides that, we have already had good experiences with these tools during exercises. Pymupdf is one of the common libraties for parsing PDF files and bibtexparser is also used for parsing bibtex references. The other two libraries were the best know choices for the respective tasks.



\subsubsection{LLM Model}
\label{choice:model}

To select models for our system we used the following criteria:

\begin{itemize}
	\item All models should be available without payment at least for non-commercial project  (as required by exam task statement).
	\item Every model must be available at inference endpoints of either HuggingFace or Groq within free tiers.
	\item The model used for the processing of paper texts must have context window larger than 30 000 tokens (because the longest paper among chosen amounts for about 15 000 tokens and we wanted to have some slack to be sure, besides, 32 000 is one of the commonly used context lengths).
	\item The model for tool-equipped agents must be tuned or at least allow for function calling (for the exam task requires at least some agents to use tools and such model would allow tool usage with lesser amount of tinkering).
	\item The model for tool-equipped agents must be compatible with AutoGen framework (for this is the framework of choice, as described in~\ref{choice:software}.
	\item Optionally, the model used for the processing of paper texts should be adapted for summarization. Even more preferrable, if it's trained for summarization using materials like the scientific datasets used in~\cite{Cohan_2018}). This requirement is optional, because text-to-text generating models can be used for summarizations to some extent even without specialized training, for this a one of the possible text-to-text tasks. At the same time, the exam task does not state that the model must be specifically targeted for text summarization, but that it must be cabaple of it.
\end{itemize}

We started by looking for a model for paper processing. Using the model search interface of~\cite{huggingface}, we have set the following search parameters: 

\begin{enumerate}
	\item Tasks: Summarization or Text2Text generation
	\item Other: HF Inference API
	\item Language: we tried setting english language, but this has limited the search too much and excluded seemingly useful models that lack english language in model tags despite supporting it
	\item Datasets: setting the ''tasks`` and ''other`` filters as described was limiting the search so much that we have omitted this filter
	\item License: setting the ''tasks`` and ''other`` filters as described was limiting the search so much that we have omitted this filter
	\item Libraries: since the filter does not have AutoGen framework, we left it empty
\end{enumerate}

This has left us no more than 10 possible models. After examining the model cards as well as configuration files of the model, we did not find any with a sufficient context length. The largest specified context length (4096) had the model ''google/bigbird-pegasus-large-arxiv`` used in~\cite{zaheer2021big}. For some models neither the card nor the files has given any reliable information. This forced us to further relax the filtering by also using ''Text generation`` in Tasks filter. The new filter configuration showed 48 possible models. We have examined the model cards and configurations one by one until we found 3 promising candidates:

\begin{itemize}
	\item deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B with 128K context window
	\item meta-llama/Llama-3.3-70B-Instruct with 128K context window, instruction tuned, also supports tool usage
	\item mistralai/Mistral-7B-Instruct-v0.3 with 32K context window, instruction tuned, supports tool usage
\end{itemize}

From these three we chose the latter, because it has a sufficient context length, supports tool usage and is smaller than the second one. The smaller size model is favourable in this case, because request processing on the same computation architecture takes less time. The chosen model is also instruction tuned, that, as we supposed, can enhance results in our case.

Unfortunately, it seems that the inference endpoint of HuggingFace is incompatible with reflect-on-tool-usage feature of AutoGen OpenAIChatCompletionClient: as soon as we were having the feature on, we were getting an error regarding bad response format. This is why we decided to choose Groq as the inference provider for tool equipped agents. It is important to mention that despite this decision, we still use HuggingFace for paper processing. This is due to the free tier limits of Groq that do not allow the required number of tokens to be processed.

On Groq, multiple models are viable candidates for the task, at least the following:

\begin{itemize}
	\item gemma2-9b-it
	\item llama-3.1-8b-instant
	\item llama3-8b-8192
	\item llama3-70b-8192
\end{itemize}

Two smallest llama models both seemed to be good candidates for the promise of faster responses. However, to process more complex conversations a larger model could be better. Besides, llama3-70b-8192 was shown to work well for multi-agent orchestration with tool use. This way we chose the latter model. 

\paragraph{Final choice:}

\begin{itemize}
	\item for processing of paper texts we use mistralai/Mistral-7B-Instruct-v0.3 through the inference endpoint of HuggingFace;
	\item for tool usage and decision making we use llama-3.1-8b-instant through Groq.
\end{itemize}


\subsubsection{Research papers}

We were interested in multi-agent LLM-based systems for end-to-end software development -- such frameworks, that, given software requirements, emulate real world software development process and deliver a working application. Such software process  may include not only coding, but also other software process activities, such as refinement of requirements, project management, creation of of interconnected files, quality assurance and other activities. 

We searched on Google Scholar\footnote{https://scholar.google.com/}, Arxiv, Katalog+\footnote{https://hbz-rptu.primo.exlibrisgroup.com/discovery/search?vid=49HBZ\_RTU:RPTU} and also DBLP\footnote{https://dblp.org}. We used the following search terms: multi agent, llm, software development. In the case where search engines allowed that, we searched in titles and abstracts. We have also limited the year with which the papers were dated with the timeframe of 2020 to 2024, where the search engines have allowed that. The searched resulted in a multitude of papers of which only a small amount falls into our area of interest. 

\cite{he2024llm}, present in the results of multiple search databases, have caught our attention. This work, dated end 2024, reviews literature in LLM-based multi-agent systems for software engineering. Among other reviewed topics, it also write on multiple frameworks that relate to our interest. We used some of them for this work and also include work that was not reviewed in this article.


The total amount of candidate papers was higher than the required 6 and we have used the following criteria to chose:

\begin{itemize}
	
	\item As stated above, all selected papers must be in the field of multi-agent llm-based frameworks for end-to-end software development 	 
	\item No work should have used any of the frameworks that other selected papers have contributed as a component. This is motivated by our interest in different and comparable frameworks.
	\item Every work should include evaluation using at least one publicly available benchmark. Best case: all the papers use the same benchmarks to allow comparison of their efficiency.
	\item Selected papers must cover at least 2 software development process paradigms: waterfall and agile.
	\item All papers must be available on Arxiv to allow easy downloading by agents.
\end{itemize}

Application of this criteria has still left some freedom of choice and we have arbitrarily selected the following frameworks:


\begin{enumerate}
	\item MetaGPT \cite{hong2024metagptmetaprogrammingmultiagent}  (waterfall)
	\item ChatDev \cite{qian2024chatdevcommunicativeagentssoftware} (waterfall)
	\item CodePori \cite{rasheed2024codeporilargescaleautonomoussoftware} (waterfall) 
	\item AgileGen \cite{zhang2024empoweringagilebasedgenerativesoftware} (agile)
	\item AgileCoder \cite{nguyen2024agilecoderdynamiccollaborativeagents} (agile)
	\item EvoMAC \cite{hu2024selfevolvingmultiagentcollaborationnetworks} (dynamic process)
\end{enumerate}

At least one of the first two, MetaGPT and ChatDev, is used in other selected works as a comparison competitor, this has motivated these two choices. AgileGen and AgileCoder were the only frameworks for agile software process that we have found. We have included EvoMAC because it adopts a method similar to backpropagation from the realm of neural network training to update the development process so that the set of agents used and the topology of their communication may be changed multiple times during one run automatically.


Some of the papers are preprints, but include interesting contributions and since we are not writing this literature review for publication but only for examination and testing of our system, we consider this acceptable.

\subsubsection{Evaluation metric}

For numerical evaluation of literature reviews we have devised the following dimensions so that the maximum valuation is 100 and every dimension can contribute only a part of it:


\paragraph{Conceptual (up to 80 points)}
\begin{enumerate}
	\item Clearance of research question (up to 5 points)
	\item Quality of paper summarizations (up to 10 point \textbf{per paper}): 
		\begin{itemize}
			\item Share of contributions covered (up to 5 points)
			\item Depth of contributions covered (up to 5 points)
		\end{itemize}
	\item Quality of critical reflection in alignment with the research question
		\begin{itemize}
			\item Quality of critical reflection (up to 10 points)
			\item Alignment of reflection with the stated research question (up to 5 points)
		\end{itemize}
\end{enumerate}

\paragraph{Structural (up to 20 points)}
\begin{enumerate}
	\setcounter{enumi}{4}
	\item Quality of introduction (up to 5 points)
	\item Clearance of structure and logical connectedness through the whole review  (up to 5 points)
	\item Coverage of relationships between papers (up to 5 points)
	\item Quality of conclusion (up to  5points)
\end{enumerate}

Besides that, we set a disqualification criteria on dimensions 2 and 3 so that for a review to be qualified for evaluation, it must go into at least 4 out of 6 possible papers and include critical reflection. Reviews not passing this criteria are disqualified.

This design allows comparison on every dimension as well as on the basis of the whole valuation in terms of hoch much one review is better than another.







